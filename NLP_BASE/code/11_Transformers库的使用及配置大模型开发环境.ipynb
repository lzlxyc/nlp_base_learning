{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9cfe981-9740-41e7-8725-8fe263159c88",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# <center>本地部署开源大模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df29c64-4f00-49a2-ab97-2ecc1a97be6f",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## <center>Ch.11 大模型开发工具Transformers库的使用及配置大模型开发环境"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddf3b09-59b7-41df-a4e9-b03af9ea474b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;本节课的内容重点，是和大家一起探讨一下大模型开发库 Hugging Face Transformers库的使用以及大模型开发环境的搭建。\n",
    "\n",
    "&emsp;&emsp;截止目前，对于Qwen开源系列模型，我们已经掌握双系统下的私有化部署方法，而每个参数量下的大模型其对应的GitHub项目文件，为用户提供的一些快速启动该模型的代码逻辑和脚本，也能够顺利的运行。比如之前介绍的直接运行`web_demo.py`就可以启动Web UI的交互页面，直接运行`openai_api.py`就可以启动一个服务接口，支持远程调用。\n",
    "\n",
    "&emsp;&emsp;但透过表面看本质，官方提供的Demo级别的代码，对于私有化大模型做上层应用开发的需求来说，大家应该也能感受到是肯定是无法直接应用的。私有化业务，必然要涉及到更高级的代码封装，比如异步、高性能计算，数据的处理等高阶优化操作，特别是：目前有非常多的推理加速开源框架，其一是能节省显存，其二又能加速推理效率，那么如果去将自己使用的开源大模型集成到某个框架中，并且基于此做更高级的抽象，也必然是实际工作中亟需的工作。所以如何去理解大模型的启动过程，参与推理和训练每个阶段的关键点，我们需要有一个比较明确的认识。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3eb85a-c170-4a34-a2f1-86e78f58dfb3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;本节内容，我们就来一点一点的撕破大模型推理的背后原理，通过Transformers库和具体的代码，来理解从用户输入Prompt到Qwen模型中，最终返回模型回复，都需要经过哪些主要的过程，最后手动实现Qwen-Chat类模型对vLLM开源框架的接入。\n",
    "\n",
    "&emsp;&emsp;首先，当我们尝试一个新的开源大模型的时候，往往看到的都是这样的官方说明："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2fd056-3a86-4bb7-8289-161ac7395def",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- **ChatGLM3-6B：https://github.com/THUDM/ChatGLM3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f80672-c8fa-43c6-9d3c-f1b1756f5f21",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401311022764.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f1a699-bd58-4fd8-a32e-acc44d0ef108",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- **Qwen-7B：https://github.com/QwenLM/Qwen/blob/main/README_CN.md**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ff80e3-f440-46b0-afb0-05889686a318",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401311024443.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046018b1-aa6f-4219-b606-cb1b825f8c34",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;可以看到，根据官方的描述，仅仅需要几行代码就可以建立起与某个开源大模型的对话交互。我们可以以Qwen-7B-Chat模型为例，随便打开一个可以运行Python的IDE来尝试运行。我这里使用Jupyter Lab："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a31c317-deb2-4dc4-9792-1a9f189caacb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;看似简单的几行代码，却往往难住很多人无法成功运行的原因，主要是以下两点："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e5d62c-3f2a-4b51-b8a8-c2c43d24de8d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- **问题1：没有安装Pytorch** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fe6475-d6b9-4f76-9d87-f246e7185741",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;关于如何安装Pytorch，我们在本地部署大模型的每一节内容中都有详细的介绍，此处不再重复说明。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b236d2-5455-4fa6-93b1-42742c3cdb56",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- **问题2：No module named 'transformers** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feb0a908-2c83-4228-a900-20a422125830",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgeneration\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m GenerationConfig\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# 可选的模型包括: \"Qwen/Qwen-7B-Chat\", \"Qwen/Qwen-14B-Chat\"\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig\n",
    "\n",
    "# 可选的模型包括: \"Qwen/Qwen-7B-Chat\", \"Qwen/Qwen-14B-Chat\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True).eval()\n",
    "\n",
    "model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)\n",
    "\n",
    "# 第一轮对话\n",
    "response, history = model.chat(tokenizer, \"你好\", history=None)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee990ab-462c-40d9-bf59-03cf35893b42",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;问题在于缺少transformers模块，也就是导致这两行代码不能执行：\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f2d2d9-7b2c-418e-b883-a6befddd36ad",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;对应的解决办法是：使用Python的包管理工具pip,来安装transformers库。代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "266bec04-f289-49e6-ac6d-3d84c2992f74",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.32.0\n",
      "  Obtaining dependency information for transformers==4.32.0 from https://files.pythonhosted.org/packages/ae/95/283a1c004430bd2a9425d6937fc545dd49a4e4592feb76be0299a14e2378/transformers-4.32.0-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.32.0-py3-none-any.whl.metadata (118 kB)\n",
      "     ---------------------------------------- 0.0/118.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/118.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/118.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/118.5 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/118.5 kB ? eta -:--:--\n",
      "     --------- --------------------------- 30.7/118.5 kB 330.3 kB/s eta 0:00:01\n",
      "     ------------ ------------------------ 41.0/118.5 kB 281.8 kB/s eta 0:00:01\n",
      "     ------------------- ----------------- 61.4/118.5 kB 328.2 kB/s eta 0:00:01\n",
      "     ------------------------- ----------- 81.9/118.5 kB 383.3 kB/s eta 0:00:01\n",
      "     ---------------------------- -------- 92.2/118.5 kB 350.1 kB/s eta 0:00:01\n",
      "     ------------------------------------ 118.5/118.5 kB 408.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in d:\\software\\anaconda3\\lib\\site-packages (from transformers==4.32.0) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in d:\\software\\anaconda3\\lib\\site-packages (from transformers==4.32.0) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\software\\anaconda3\\lib\\site-packages (from transformers==4.32.0) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\software\\anaconda3\\lib\\site-packages (from transformers==4.32.0) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\software\\anaconda3\\lib\\site-packages (from transformers==4.32.0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\software\\anaconda3\\lib\\site-packages (from transformers==4.32.0) (2022.7.9)\n",
      "Requirement already satisfied: requests in d:\\software\\anaconda3\\lib\\site-packages (from transformers==4.32.0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in d:\\software\\anaconda3\\lib\\site-packages (from transformers==4.32.0) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in d:\\software\\anaconda3\\lib\\site-packages (from transformers==4.32.0) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\software\\anaconda3\\lib\\site-packages (from transformers==4.32.0) (4.65.0)\n",
      "Requirement already satisfied: fsspec in d:\\software\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.32.0) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\software\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.32.0) (4.7.1)\n",
      "Requirement already satisfied: colorama in d:\\software\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers==4.32.0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\software\\anaconda3\\lib\\site-packages (from requests->transformers==4.32.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\software\\anaconda3\\lib\\site-packages (from requests->transformers==4.32.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\software\\anaconda3\\lib\\site-packages (from requests->transformers==4.32.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\software\\anaconda3\\lib\\site-packages (from requests->transformers==4.32.0) (2023.11.17)\n",
      "Downloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n",
      "   ---------------------------------------- 0.0/7.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/7.5 MB 1.9 MB/s eta 0:00:04\n",
      "   ---------------------------------------- 0.1/7.5 MB 1.3 MB/s eta 0:00:06\n",
      "    --------------------------------------- 0.1/7.5 MB 1.2 MB/s eta 0:00:07\n",
      "    --------------------------------------- 0.2/7.5 MB 1.1 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.2/7.5 MB 1.1 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.3/7.5 MB 1.1 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.4/7.5 MB 1.3 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.5/7.5 MB 1.4 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 0.6/7.5 MB 1.6 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 0.8/7.5 MB 1.8 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.0/7.5 MB 2.0 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.2/7.5 MB 2.3 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 1.6/7.5 MB 2.7 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 1.9/7.5 MB 3.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.4/7.5 MB 3.6 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.0/7.5 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.6/7.5 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.6/7.5 MB 5.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 5.2/7.5 MB 6.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 5.6/7.5 MB 6.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.1/7.5 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.4/7.5 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.5/7.5 MB 7.4 MB/s eta 0:00:00\n",
      "Installing collected packages: transformers\n",
      "Successfully installed transformers-4.32.0\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers==4.32.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2405bb-1aa4-47ff-96af-161c435cf5f5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;然后再次尝试执行："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f80b94-49fc-4e9c-a2b8-5615de6a61f1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- **问题3：We couldn't connect to 'https://huggingface.co' to load this file ....** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbb70567-1f53-4fe4-a686-43bef37f329b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen-7B-Chat/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x00000146AEFFBB50>, 'Connection to huggingface.co timed out. (connect timeout=10)'))' thrown while requesting HEAD https://huggingface.co/Qwen/Qwen-7B-Chat/resolve/main/tokenizer_config.json\n",
      "'HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen-7B-Chat/resolve/main/tokenization_qwen.py (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x00000146B55CFF10>, 'Connection to huggingface.co timed out. (connect timeout=10)'))' thrown while requesting HEAD https://huggingface.co/Qwen/Qwen-7B-Chat/resolve/main/tokenization_qwen.py\n",
      "'HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen-7B-Chat/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x00000146AEEDF7D0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))' thrown while requesting HEAD https://huggingface.co/Qwen/Qwen-7B-Chat/resolve/main/config.json\n",
      "'HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen-7B-Chat/resolve/main/configuration_qwen.py (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x00000146AEEDD6D0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))' thrown while requesting HEAD https://huggingface.co/Qwen/Qwen-7B-Chat/resolve/main/configuration_qwen.py\n",
      "'HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen-7B-Chat/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x00000146B4F1A050>, 'Connection to huggingface.co timed out. (connect timeout=10)'))' thrown while requesting HEAD https://huggingface.co/Qwen/Qwen-7B-Chat/resolve/main/config.json\n",
      "'HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen-7B-Chat/resolve/main/modeling_qwen.py (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x00000146B0123490>, 'Connection to huggingface.co timed out. (connect timeout=10)'))' thrown while requesting HEAD https://huggingface.co/Qwen/Qwen-7B-Chat/resolve/main/modeling_qwen.py\n",
      "Could not locate the modeling_qwen.py inside Qwen/Qwen-7B-Chat.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like Qwen/Qwen-7B-Chat is not the path to a directory containing a file named modeling_qwen.py.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mLocalEntryNotFoundError\u001B[0m                   Traceback (most recent call last)",
      "File \u001B[1;32mD:\\software\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py:428\u001B[0m, in \u001B[0;36mcached_file\u001B[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[0;32m    426\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    427\u001B[0m     \u001B[38;5;66;03m# Load from URL or cache if already cached\u001B[39;00m\n\u001B[1;32m--> 428\u001B[0m     resolved_file \u001B[38;5;241m=\u001B[39m hf_hub_download(\n\u001B[0;32m    429\u001B[0m         path_or_repo_id,\n\u001B[0;32m    430\u001B[0m         filename,\n\u001B[0;32m    431\u001B[0m         subfolder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(subfolder) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m subfolder,\n\u001B[0;32m    432\u001B[0m         repo_type\u001B[38;5;241m=\u001B[39mrepo_type,\n\u001B[0;32m    433\u001B[0m         revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[0;32m    434\u001B[0m         cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[0;32m    435\u001B[0m         user_agent\u001B[38;5;241m=\u001B[39muser_agent,\n\u001B[0;32m    436\u001B[0m         force_download\u001B[38;5;241m=\u001B[39mforce_download,\n\u001B[0;32m    437\u001B[0m         proxies\u001B[38;5;241m=\u001B[39mproxies,\n\u001B[0;32m    438\u001B[0m         resume_download\u001B[38;5;241m=\u001B[39mresume_download,\n\u001B[0;32m    439\u001B[0m         token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[0;32m    440\u001B[0m         local_files_only\u001B[38;5;241m=\u001B[39mlocal_files_only,\n\u001B[0;32m    441\u001B[0m     )\n\u001B[0;32m    442\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m GatedRepoError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32mD:\\software\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    116\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[1;32m--> 118\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\software\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1291\u001B[0m, in \u001B[0;36mhf_hub_download\u001B[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001B[0m\n\u001B[0;32m   1290\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1291\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m LocalEntryNotFoundError(\n\u001B[0;32m   1292\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConnection error, and we cannot find the requested files in\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1293\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m the disk cache. Please try again or make sure your Internet\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1294\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m connection is on.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1295\u001B[0m         )\n\u001B[0;32m   1297\u001B[0m \u001B[38;5;66;03m# From now on, etag and commit_hash are not None.\u001B[39;00m\n",
      "\u001B[1;31mLocalEntryNotFoundError\u001B[0m: Connection error, and we cannot find the requested files in the disk cache. Please try again or make sure your Internet connection is on.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# 可选的模型包括: \"Qwen/Qwen-7B-Chat\", \"Qwen/Qwen-14B-Chat\"\u001B[39;00m\n\u001B[0;32m      5\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mQwen/Qwen-7B-Chat\u001B[39m\u001B[38;5;124m\"\u001B[39m, trust_remote_code\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m----> 7\u001B[0m model \u001B[38;5;241m=\u001B[39m AutoModelForCausalLM\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mQwen/Qwen-7B-Chat\u001B[39m\u001B[38;5;124m\"\u001B[39m, device_map\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m, trust_remote_code\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;241m.\u001B[39meval()\n\u001B[0;32m      9\u001B[0m model\u001B[38;5;241m.\u001B[39mgeneration_config \u001B[38;5;241m=\u001B[39m GenerationConfig\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mQwen/Qwen-7B-Chat\u001B[39m\u001B[38;5;124m\"\u001B[39m, trust_remote_code\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# 第一轮对话\u001B[39;00m\n",
      "File \u001B[1;32mD:\\software\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:503\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m    501\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_remote_code \u001B[38;5;129;01mand\u001B[39;00m trust_remote_code:\n\u001B[0;32m    502\u001B[0m     class_ref \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mauto_map[\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m]\n\u001B[1;32m--> 503\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m get_class_from_dynamic_module(\n\u001B[0;32m    504\u001B[0m         class_ref, pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    505\u001B[0m     )\n\u001B[0;32m    506\u001B[0m     _ \u001B[38;5;241m=\u001B[39m hub_kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcode_revision\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    507\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misdir(pretrained_model_name_or_path):\n",
      "File \u001B[1;32mD:\\software\\anaconda3\\Lib\\site-packages\\transformers\\dynamic_module_utils.py:485\u001B[0m, in \u001B[0;36mget_class_from_dynamic_module\u001B[1;34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001B[0m\n\u001B[0;32m    483\u001B[0m     code_revision \u001B[38;5;241m=\u001B[39m revision\n\u001B[0;32m    484\u001B[0m \u001B[38;5;66;03m# And lastly we get the class inside our newly created module\u001B[39;00m\n\u001B[1;32m--> 485\u001B[0m final_module \u001B[38;5;241m=\u001B[39m get_cached_module_file(\n\u001B[0;32m    486\u001B[0m     repo_id,\n\u001B[0;32m    487\u001B[0m     module_file \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.py\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    488\u001B[0m     cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[0;32m    489\u001B[0m     force_download\u001B[38;5;241m=\u001B[39mforce_download,\n\u001B[0;32m    490\u001B[0m     resume_download\u001B[38;5;241m=\u001B[39mresume_download,\n\u001B[0;32m    491\u001B[0m     proxies\u001B[38;5;241m=\u001B[39mproxies,\n\u001B[0;32m    492\u001B[0m     token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[0;32m    493\u001B[0m     revision\u001B[38;5;241m=\u001B[39mcode_revision,\n\u001B[0;32m    494\u001B[0m     local_files_only\u001B[38;5;241m=\u001B[39mlocal_files_only,\n\u001B[0;32m    495\u001B[0m     repo_type\u001B[38;5;241m=\u001B[39mrepo_type,\n\u001B[0;32m    496\u001B[0m )\n\u001B[0;32m    497\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m get_class_in_module(class_name, final_module\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.py\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "File \u001B[1;32mD:\\software\\anaconda3\\Lib\\site-packages\\transformers\\dynamic_module_utils.py:292\u001B[0m, in \u001B[0;36mget_cached_module_file\u001B[1;34m(pretrained_model_name_or_path, module_file, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[0;32m    289\u001B[0m new_files \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    290\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    291\u001B[0m     \u001B[38;5;66;03m# Load from URL or cache if already cached\u001B[39;00m\n\u001B[1;32m--> 292\u001B[0m     resolved_module_file \u001B[38;5;241m=\u001B[39m cached_file(\n\u001B[0;32m    293\u001B[0m         pretrained_model_name_or_path,\n\u001B[0;32m    294\u001B[0m         module_file,\n\u001B[0;32m    295\u001B[0m         cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[0;32m    296\u001B[0m         force_download\u001B[38;5;241m=\u001B[39mforce_download,\n\u001B[0;32m    297\u001B[0m         proxies\u001B[38;5;241m=\u001B[39mproxies,\n\u001B[0;32m    298\u001B[0m         resume_download\u001B[38;5;241m=\u001B[39mresume_download,\n\u001B[0;32m    299\u001B[0m         local_files_only\u001B[38;5;241m=\u001B[39mlocal_files_only,\n\u001B[0;32m    300\u001B[0m         token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[0;32m    301\u001B[0m         revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[0;32m    302\u001B[0m         repo_type\u001B[38;5;241m=\u001B[39mrepo_type,\n\u001B[0;32m    303\u001B[0m         _commit_hash\u001B[38;5;241m=\u001B[39m_commit_hash,\n\u001B[0;32m    304\u001B[0m     )\n\u001B[0;32m    305\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_local \u001B[38;5;129;01mand\u001B[39;00m cached_module \u001B[38;5;241m!=\u001B[39m resolved_module_file:\n\u001B[0;32m    306\u001B[0m         new_files\u001B[38;5;241m.\u001B[39mappend(module_file)\n",
      "File \u001B[1;32mD:\\software\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py:468\u001B[0m, in \u001B[0;36mcached_file\u001B[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[0;32m    466\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _raise_exceptions_for_missing_entries \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _raise_exceptions_for_connection_errors:\n\u001B[0;32m    467\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 468\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[0;32m    469\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWe couldn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt connect to \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mHUGGINGFACE_CO_RESOLVE_ENDPOINT\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m to load this file, couldn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt find it in the\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    470\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m cached files and it looks like \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is not the path to a directory containing a file named\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    471\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfull_filename\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mCheckout your internet connection or see how to run the library in offline mode at\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    472\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://huggingface.co/docs/transformers/installation#offline-mode\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    473\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m    474\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m EntryNotFoundError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    475\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _raise_exceptions_for_missing_entries:\n",
      "\u001B[1;31mOSError\u001B[0m: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like Qwen/Qwen-7B-Chat is not the path to a directory containing a file named modeling_qwen.py.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig\n",
    "\n",
    "# 可选的模型包括: \"Qwen/Qwen-7B-Chat\", \"Qwen/Qwen-14B-Chat\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True).eval()\n",
    "\n",
    "model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)\n",
    "\n",
    "# 第一轮对话\n",
    "response, history = model.chat(tokenizer, \"你好\", history=None)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d860dd-0571-4ac1-970a-26b16828f44e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;从报错信息上看，问题的原因在于，无法从'https://huggingface.co' 这个网站中，拉取到`Qwen/Qwen-7B-Chat`这个模型导致的无法运行，而 'https://huggingface.co' 这个网站，对于国内的网络是不通的。所以需要开启科学上网的环境，打开后是这样的一个网站：（在本机先开启科学上网后再进行尝试）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24db421-2851-4030-89e3-e5a63454dcfd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401311058143.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefe6959-0bc5-4d6d-9e67-13d6db008539",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;解决的方式是：在本机环境下开启科学上网，然后再执行这段代码。前提是：需要额外在安装如下两个依赖包："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb9b5163-073d-4725-a4e0-77d307c12025",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers_stream_generator in d:\\software\\anaconda3\\lib\\site-packages (0.0.4)\n",
      "Requirement already satisfied: transformers>=4.26.1 in d:\\software\\anaconda3\\lib\\site-packages (from transformers_stream_generator) (4.32.0)\n",
      "Requirement already satisfied: filelock in d:\\software\\anaconda3\\lib\\site-packages (from transformers>=4.26.1->transformers_stream_generator) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in d:\\software\\anaconda3\\lib\\site-packages (from transformers>=4.26.1->transformers_stream_generator) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\software\\anaconda3\\lib\\site-packages (from transformers>=4.26.1->transformers_stream_generator) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\software\\anaconda3\\lib\\site-packages (from transformers>=4.26.1->transformers_stream_generator) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\software\\anaconda3\\lib\\site-packages (from transformers>=4.26.1->transformers_stream_generator) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\software\\anaconda3\\lib\\site-packages (from transformers>=4.26.1->transformers_stream_generator) (2022.7.9)\n",
      "Requirement already satisfied: requests in d:\\software\\anaconda3\\lib\\site-packages (from transformers>=4.26.1->transformers_stream_generator) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in d:\\software\\anaconda3\\lib\\site-packages (from transformers>=4.26.1->transformers_stream_generator) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in d:\\software\\anaconda3\\lib\\site-packages (from transformers>=4.26.1->transformers_stream_generator) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\software\\anaconda3\\lib\\site-packages (from transformers>=4.26.1->transformers_stream_generator) (4.65.0)\n",
      "Requirement already satisfied: fsspec in d:\\software\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers>=4.26.1->transformers_stream_generator) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\software\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers>=4.26.1->transformers_stream_generator) (4.7.1)\n",
      "Requirement already satisfied: colorama in d:\\software\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers>=4.26.1->transformers_stream_generator) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\software\\anaconda3\\lib\\site-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\software\\anaconda3\\lib\\site-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\software\\anaconda3\\lib\\site-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\software\\anaconda3\\lib\\site-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers_stream_generator einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98895e9c-2466-4118-b6b2-fc80ca76510c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Obtaining dependency information for accelerate from https://files.pythonhosted.org/packages/a6/b9/44623bdb05595481107153182e7f4b9f2ef9d3b674938ad13842054dcbd8/accelerate-0.26.1-py3-none-any.whl.metadata\n",
      "  Downloading accelerate-0.26.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\software\\anaconda3\\lib\\site-packages (from accelerate) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\software\\anaconda3\\lib\\site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in d:\\software\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in d:\\software\\anaconda3\\lib\\site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in d:\\software\\anaconda3\\lib\\site-packages (from accelerate) (2.1.1)\n",
      "Requirement already satisfied: huggingface-hub in d:\\software\\anaconda3\\lib\\site-packages (from accelerate) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in d:\\software\\anaconda3\\lib\\site-packages (from accelerate) (0.3.2)\n",
      "Requirement already satisfied: filelock in d:\\software\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in d:\\software\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
      "Requirement already satisfied: sympy in d:\\software\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in d:\\software\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in d:\\software\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in d:\\software\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (2023.4.0)\n",
      "Requirement already satisfied: requests in d:\\software\\anaconda3\\lib\\site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\software\\anaconda3\\lib\\site-packages (from huggingface-hub->accelerate) (4.65.0)\n",
      "Requirement already satisfied: colorama in d:\\software\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\software\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\software\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\software\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\software\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->accelerate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\software\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\software\\anaconda3\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n",
      "   ---------------------------------------- 0.0/270.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/270.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/270.9 kB ? eta -:--:--\n",
      "   ---- ---------------------------------- 30.7/270.9 kB 435.7 kB/s eta 0:00:01\n",
      "   ----- --------------------------------- 41.0/270.9 kB 281.8 kB/s eta 0:00:01\n",
      "   ----- --------------------------------- 41.0/270.9 kB 281.8 kB/s eta 0:00:01\n",
      "   -------- ------------------------------ 61.4/270.9 kB 297.7 kB/s eta 0:00:01\n",
      "   ------------- ------------------------- 92.2/270.9 kB 350.1 kB/s eta 0:00:01\n",
      "   -------------------- ----------------- 143.4/270.9 kB 448.2 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 174.1/270.9 kB 476.3 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 225.3/270.9 kB 550.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- 270.9/270.9 kB 617.6 kB/s eta 0:00:00\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.26.1\n"
     ]
    }
   ],
   "source": [
    "! pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5902470-a2ea-4bff-9070-495b95736e81",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;**注意：安装完上述两个依赖包后，需要重启Jupyter Lab才会生效。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f718c657-e8ee-40a2-abbb-49529d65daf9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f08bc2c9158461189d5c3968a08630e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)fetensors.index.json:   0%|          | 0.00/19.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snowb\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\snowb\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a821dded0769405d91e41fd94c9840c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e423dd146f42d0b81bdeaa10a15a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f77d9cc036b4c3c9f11d40ac61e9074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00008.safetensors:   0%|          | 0.00/2.02G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b504c2b08de49edbcb2374891321f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00008.safetensors:   0%|          | 0.00/2.02G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37f69c5465944168e6a2045ee94a821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00008.safetensors:   0%|          | 0.00/2.02G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32c6cfef3537410d8d8c3f6f2eb19b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00008.safetensors:   0%|          | 0.00/2.02G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9953885bba924cdfa23424d10c4c8fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00008.safetensors:   0%|          | 0.00/2.02G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2560af32836a40c79ba8c65ab8e0abb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00008.safetensors:   0%|          | 0.00/2.02G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b129064f61344d3b244181f22d3022f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653c6d9bd5874bc59724066b17be569e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d616d792c5414cbe5aff5d296e458f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/273 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！有什么我能帮你的吗？\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig\n",
    "\n",
    "# 可选的模型包括: \"Qwen/Qwen-7B-Chat\", \"Qwen/Qwen-14B-Chat\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True).eval()\n",
    "\n",
    "\n",
    "\n",
    "model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)\n",
    "\n",
    "# 第一轮对话\n",
    "response, history = model.chat(tokenizer, \"你好\", history=None)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5a9d7c0-0c05-46ef-8ca2-3c1c8071f8e0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是来自阿里云的大规模语言模型，我叫通义千问。我的主要功能是帮助用户生成与给定词语相关的高质量文本，以满足各种应用场景的需求。\n",
      "\n",
      "我可以根据不同的任务和场景，生成各种类型的文本，包括但不限于文章、故事、诗歌、新闻、报告、评论、指南、教程、代码等。无论你需要的是专业性强的内容，还是富有创意的文学作品，我都可以帮你轻松实现。\n",
      "\n",
      "除此之外，我还具备多种能力，例如：回答问题、提供定义、解释和建议、将文本从一种语言翻译成另一种语言、总结文本、生成文本、写故事、分析情绪、提供建议、开发算法、编写代码等。\n",
      "\n",
      "在使用过程中，我会不断学习和进步，不断提升自己的表现，以更好地为用户提供服务。如果您有任何问题或需要帮助，请随时告诉我，我会尽力为您提供支持。\n"
     ]
    }
   ],
   "source": [
    "# 第一轮对话\n",
    "response, history = model.chat(tokenizer, \"请你详细的介绍一下你自己\", history=None)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a62c14e-977b-4efe-909b-1f9d562de2bb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;如果顺利执行后，会是非常漫长的下载过程，这个过程下载的文件，就是我们在前期课程中本地部署Qwen模型中从 Hugging Face 或者 modelscope 社区下载到的模型权重。只不过，通过上述代码，会自动的从Hugging Face上下载到了本地的`C:\\Users\\Admin\\.cache\\huggingface\\hub.` 这个文件中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78edc67b-385f-4291-9bc0-b4c33120cb18",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401311123636.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47114d5-2eb6-4cd0-9d8c-b5db635258ba",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;下载到本地后，下次启动默认就会从Cache中加载模型，不会重复下载。此时再执行代码就可以正常的完成输出，而如果出现如下错误，则说明本机的配置太低，不足以支撑Qwen-7B-Chat模型的运行条件，会直接导致当前的开发环境进程被强制终止："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0ab2ae-a579-45ca-a693-820df19297dd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401311126074.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1d5d3e-cf54-4da8-a5ea-13bcf62ce5dc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;解决办法只能是选择更高配置的机器来尝试。或者选择去加载更小的模型，比如Qwen的1.8B模型，但是官方也说明了，上述加载方式可选的模型仅包括: \"Qwen/Qwen-7B-Chat\" 和 \"Qwen/Qwen-14B-Chat\"，所以，如何去考虑自定义加载不同参数量的模型，也往往是很多人在无法解决硬件问题的迫切需求，同时这也是在实际的业务开发中，在大模型领域必须掌握的底层知识。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333275e4-2477-4a93-8015-0d16d6a59681",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;接下来，我们就来拆解一下为什么能通过上述几行代码，仅仅通过修改模型的名称，就可以加载到不同的大模型实现对话交互。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ec9a27-1456-4989-8ef8-7377d7e41c88",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. 大模型开发工具Transformers库"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3a8668-aef7-4c23-85f9-8b7cc70e941a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;其实从我们上面在初次与大模型建立交互过程中可能出现的报错内容来看，起决定因素的在这样两行代码中：\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15512a0-2851-4ed5-a4d4-cc88849d5c64",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;这里导入的Transformers，是一个 Python库，可以允许用户下载和训练模型，最初是用于机器学习领域，而随着大模型的爆发，Hugging Face 也是迅速占领了大模型的市场，将功能扩展到包括多模态、计算机视觉和音频处理等其他用途的模型。\n",
    "\n",
    "> Github地址：https://github.com/huggingface/transformers\n",
    "\n",
    "> Hugging Face地址：https://huggingface.co/docs/transformers/index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43672ae6-f4d5-4b92-8194-dd87584d768b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401311139436.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebd0a29-f589-4ff5-968d-eebb38ea2975",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;其价值在于：\n",
    "\n",
    "- 🤗 Transformers 提供了数以千计的预训练模型，支持 100 多种语言的文本分类、信息抽取、问答、摘要、翻译、文本生成。它的宗旨是让最先进的 NLP 技术人人易用。\n",
    "\n",
    "- 🤗 Transformers 提供了便于快速下载和使用的API，让你可以把预训练模型用在给定文本、在你的数据集上微调然后通过 model hub 与社区共享。同时，每个定义的 Python 模块均完全独立，方便修改和快速研究实验。\n",
    "\n",
    "- 🤗 Transformers 支持三个最热门的深度学习库： Jax, PyTorch 以及 TensorFlow — 并与之无缝整合。你可以直接使用一个框架训练你的模型然后用另一个加载和推理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35c6cd2-87ee-4e34-9756-9b3e7f03a206",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;简单理解，就是Transformers库，集成了大量的预训练模型，将其内部的调用方式已经做好了封装，所以才有我们看到的 模型加载、模型推理这种仅仅通过调用某个接口就可以实现。实际上调用大模型的推理是比较复杂的，只不过Transformers库在`暗处`给我们写好了而已，让我们仅仅关注如何使用，而不需要去了解复杂的原理和代码实现，从而降低模型的使用门槛。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c8adfe-5507-4f42-9257-a5dc64fbd946",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;Transformers集成了非常多的模型，在 Hugging Face：https://huggingface.co/docs/transformers/index 中可以找到详细的说明："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b867614-772a-4989-92a0-713a8c0c744b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401311156981.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659dbcd3-b896-4504-b4ae-eb61e894949f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;这里需要去关注的概念是：Pipeline。在Transformers中，每个任务都有一个关联的pipeline()，而为了进一步简化使用门槛，Transformers抽象出了一个通用的pipeline()，其中包含所有特定任务的pipelines。pipeline()会自动加载一个默认模型和一个能够进行任务推理的预处理类。它是这样的一个自动化过程："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3be026c-cdc6-44b9-b8ea-648ef91e2a48",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401311246034.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80440505-d652-4de6-8e12-062fb7e9b352",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;这个过程的理解是：每个模型，根据其架构和训练数据，其输入数据和输出数据都是不一样的， Pre-Precessing过程，就是数据进入模型前的预处理，而Post-Precessing，是指输入数据经过模型的计算推理后，以哪种方式输出，比如分类模型，那其输出就是分类的标签， 翻译任务，就是将输入的文本翻译成指定的语言。\n",
    "\n",
    "&emsp;&emsp;而Transformers所谓的通用pipeline()，就是集成不同的模型在同一套流程中，就像启动大模型一样，只需要修改模型，就可以了。接下来可以快速的测试一下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1c61bc-d70e-4897-aa71-99366ab777fa",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;pipeline API reference：https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e570770-54e0-42e2-9bee-acdd5b1b5b79",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;首先来尝试一下文本分类任务："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c71523b-c2e7-4bf3-8efd-f01180b79eee",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 导入pipeline模块\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7421e90-6dfc-47bd-aaa7-78d2eddb0836",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c3cf3afc3e430ab7ac64c9943e14d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snowb\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\snowb\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7fc0fb0c90d4bd3bccf5c6da7103351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d596bd74640f4bfda8c0cd1feb24c87c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983e9e3afd7b4557b982cff0b77e4642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 实例化\n",
    "text_task = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb5f914-c2a0-40d6-9a44-d0df1a62d192",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;如果初次使用，会自动执行下载，下载路径就是：C:\\Users\\snowb\\.cache\\huggingface\\hub。如果想自定义模型的默认下载路径，可以在执行代码前执行：\n",
    "```python\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['HF_HOME'] =pathme/hf'\n",
    "os.environ['HF_HUB_CACHE'] pathume/hf/hub'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeb75b2-f53d-42a1-a76e-1d703c942c5f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;待模型下载完成后，就可以直接利用pipeline实例调用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cb24c80-0a46-49c1-949e-688c99fd8527",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.7773503065109253}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_task(\"今天的天气可真好啊\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "661696b9-8052-4b85-9b16-0931c8a7fadb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9618343114852905}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_task(\"我特别喜欢你，你能嫁给我吗？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b665314-7331-4c7b-b7c5-09dae69c4984",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998749494552612}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_task(\"What a lovely day it is today\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804f86d5-817b-4a29-9540-8909fa635169",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;再测试一个摘要任务："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e9e1a4f-257b-4d0e-9f96-b0c5cba2401c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dead4c1dc9b3410ea50a7dc0ae08f137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf798f9b0224f96b8dda2e0f4185200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19435b7528a5461a9b9bbfaf500013d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1deb96575e9d4f7caac4a6c43eeaf3c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d806ff2b53e43639efcc19bf0d8fd4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Summarization_task = pipeline(task=\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9672f222-52ba-4659-a6bd-9ad5b044ffa7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' 春节临近，各地年货市场也开始热闹起来 .   \\xa0‘‘�’: ‘I’m sorry.’. ‘“”: ““\\xa0”. “I”   I’ll be happy to see that.”’s a happy coincidence.   ‘A happy coincidence’? ‘It was a coincidence. It was an accident.'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Summarization_task(\"春节临近，各地年货市场也开始热闹起来。从批发市场到年货大集，人们采购的热情格外高涨。 \\\n",
    "                    位于浙江金华的“华东金华农产品物流中心”是全国重要的水果集散枢纽之一，销售范围覆盖浙江省内及周边江西、福建 \\\n",
    "                    、安徽等省市。根据市场统计，最近几天的日均交易量达1.02万吨，较前一周增长5.57%。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a749ec14-85b3-478c-8525-3ae4f6b044ac",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;其实能够直观的感受到，这种方式可以让我们非常快速的应用起某个领域的模型去生成特殊的任务。我们需要做的就是去选择Transformers库中已经支持的，且效果较好的模型，然后按照它的规范去生成结果。比如你想加载指定的模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d641a76-3f11-46a0-8670-e70dd200f02b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 通过model参数来指定具体的模型\n",
    "# speech_recognizer = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a630f5a-740b-4f35-b298-18da97cff19a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;Transformers之所以能集成如此多的模型使用同一套规范来提供服务，根本原因还是在于现在优秀的模型基本都是基于Transformer架构的，因为架构是一样的，所以并不会出现太过于大的差异。其整个Pipeline()的底层处理逻辑可以抽象如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f1fff85-d121-4f59-b652-c9ce1050ccb4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998831748962402}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_task(\"This course is amazing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49859b3-bfae-4684-a696-5cebc3957623",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;如下图所示：Tokenizer部分，就是进入模型的Pre-Processing过程。我们的输入，无论是中文还是英文还是其他语言，对于模型来说，都是不认识的，所以需要Tokenizer这个过程来将自然语言做一系列的预处理工作，这包括：\n",
    "1. 将输入的Prompt 按照某种规则切分成小的Tokens；\n",
    "2. 每个小的Tokens，在该模型训练时所建立的词表，都有一个id来对应，也就是下图的input IDs\n",
    "3. 添加特殊tokens、截断、填充等操作，以满足模型的输入要求。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66825d9c-5348-465f-85e7-c624102a3fa6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401311236687.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fbba36-5d74-47f3-823c-57cc17d19ea0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> Tokenizer 参数参考：https://huggingface.co/docs/transformers/main/internal/tokenization_utils#transformers.tokenization_utils_base.PreTrainedTokenizerBase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe170a2-5268-496c-a59e-8b88d1c77403",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;对于上述的这样一种通用的Pipeline流程，Transformers 采用的是 `AutoClass` 统一接口的设计，抽象出`from_pretrained`方法来管理Tokenizer和Model，以便我们能够自动去检索到相关的模型，并读取其模型权重、配置文件和词表信息等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddf8ce06-04bb-4d4d-9744-3d182afa3241",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 可选的模型包括: \"Qwen/Qwen-7B-Chat\", \"Qwen/Qwen-14B-Chat\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3689ca69-c0c6-4a9b-b4b5-c112315f7452",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [100644, 30534, 16872, 26288, 100167, 34187, 3837, 107954, 18397, 105580, 45629, 102572, 11319], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"今天要下大雪了，过年回不去家怎么办？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "285fb29f-e117-46c5-894c-3d04c96bf0fd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "raw_inputs = ['今天天气不好。',\n",
    "              \"连续几天都会下大雪，过年回不去家怎么办？\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88f25af9-74f1-4aea-bdfd-03a98bad4917",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[100644, 104307, 101132, 1773], [104005, 101437, 101938, 16872, 26288, 100167, 3837, 107954, 18397, 105580, 45629, 102572, 11319]], 'token_type_ids': [[0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(raw_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad29a9ad-ce2b-49ad-b696-cdf81549fa9e",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [[100644, 104307, 101132, 1773], [104005, 101437, 101938, 16872, 26288, 100167, 3837, 107954, 18397, 105580, 45629, 102572, 11319]]\n",
      "token_type_ids: [[0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "attention_mask: [[1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer(raw_inputs)\n",
    "\n",
    "# 直接遍历并打印输出结果\n",
    "for key, value in output.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cacd1c0-28be-4f3b-9c11-e80bbfac5248",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "设置Padding。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b273fa47-f17b-46a5-9b6e-08f1c11e7d83",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [[151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 100644, 104307, 101132, 1773], [104005, 101437, 101938, 16872, 26288, 100167, 3837, 107954, 18397, 105580, 45629, 102572, 11319]]\n",
      "token_type_ids: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "attention_mask: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True, pad_token='<|endoftext|>', padding_side='left',)\n",
    "output = tokenizer(raw_inputs,  padding=True)\n",
    "\n",
    "# 直接遍历并打印输出结果\n",
    "for key, value in output.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa920a71-a702-48e7-b155-f8986a4cb291",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入 1:\n",
      "b'\\xe4\\xbb\\x8a\\xe5\\xa4\\xa9'(100644) b'\\xe5\\xa4\\xa9\\xe6\\xb0\\x94'(104307) b'\\xe4\\xb8\\x8d\\xe5\\xa5\\xbd'(101132) b'\\xe3\\x80\\x82'(1773) \n",
      "\n",
      "输入 2:\n",
      "b'\\xe8\\xbf\\x9e\\xe7\\xbb\\xad'(104005) b'\\xe5\\x87\\xa0\\xe5\\xa4\\xa9'(101437) b'\\xe9\\x83\\xbd\\xe4\\xbc\\x9a'(101938) b'\\xe4\\xb8\\x8b'(16872) b'\\xe5\\xa4\\xa7'(26288) b'\\xe9\\x9b\\xaa'(100167) b'\\xef\\xbc\\x8c'(3837) b'\\xe8\\xbf\\x87\\xe5\\xb9\\xb4'(107954) b'\\xe5\\x9b\\x9e'(18397) b'\\xe4\\xb8\\x8d\\xe5\\x8e\\xbb'(105580) b'\\xe5\\xae\\xb6'(45629) b'\\xe6\\x80\\x8e\\xe4\\xb9\\x88\\xe5\\x8a\\x9e'(102572) b'\\xef\\xbc\\x9f'(11319) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 假设已经有了tokenizer和raw_inputs\n",
    "output = tokenizer(raw_inputs)\n",
    "\n",
    "# 遍历每个输入字符串\n",
    "for i, input_ids in enumerate(output['input_ids']):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    print(f\"输入 {i+1}:\")\n",
    "    \n",
    "    # 创建一个字符串来存储格式化的输出\n",
    "    formatted_output = \"\"\n",
    "    for token, id in zip(tokens, input_ids):\n",
    "        formatted_output += f\"{token}({id}) \"\n",
    "    \n",
    "    print(formatted_output + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92adb83-99ee-4b55-b0f8-8b33d0254cd0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;类似 b'\\xe4\\xbb\\x8a\\xe5\\xa4\\xa9' 这样的序列是字节串表示的 UTF-8 编码的汉字。在 Python 中，以 b 前缀开头的字面量表示字节串（byte strings）。这些字节串是由文本在 UTF-8 编码下转换成的字节序列。如果想转化回汉字，则输入如下代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba12b209-46b4-44d5-b70a-9a16fcb0facc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天\n"
     ]
    }
   ],
   "source": [
    "token_bytes = b'\\xe4\\xbb\\x8a\\xe5\\xa4\\xa9'  # 汉字 \"今天\" 的 UTF-8 编码\n",
    "token_str = token_bytes.decode('utf-8')  # 解码为字符串\n",
    "print(token_str)  # 输出 \"今天\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573ba3fa-21cd-4f76-a338-03418742e3dd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;经过了Tokenizer的处理，会将每个词的Input_ids映射到具体的Embedding向量，组成一个Embedding向量矩阵，输入到大模型中，执行内部的矩阵计算。可以简单的理解如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecb684c-bf0a-4da2-9ca3-8939925174f4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401311439292.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a057ac97-7591-42b8-9610-b94d03b58772",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;经过Tokenizer后得到词的向量矩阵后，接下来进入模型，对于大模型这种自回归模型来说，其生成文本的过程是这样的："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e6ae1a-98ae-4a14-aa04-547b75a43c9a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401311443666.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe7d851-1009-44bb-b70f-8eb29f84e91c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> Transformer架构的学习地址：https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8663f8f-f526-493f-9b2a-0b9b5da147b6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401311515513.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fb0bfe-b541-48ab-88e0-6aafce9d3479",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;Head，表示的是模型头，用来接在基础模型的后面，从而将hidden states文本表示进一步处理，用于具体的任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d6e7a55-b16c-4190-8a95-f1a37c38ccce",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3bd9abc009b4e7baecc2baa9f80bb29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5769c13-91e1-4d3c-a3c0-49b4134d0e08",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QWenLMHeadModel(\n",
       "  (transformer): QWenModel(\n",
       "    (wte): Embedding(151936, 4096)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (rotary_emb): RotaryEmbedding()\n",
       "    (h): ModuleList(\n",
       "      (0-31): 32 x QWenBlock(\n",
       "        (ln_1): RMSNorm()\n",
       "        (attn): QWenAttention(\n",
       "          (c_attn): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): RMSNorm()\n",
       "        (mlp): QWenMLP(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (w2): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (c_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a23351-4b9e-4cda-b950-5ab27d2f9f43",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;可以看到，上述就是Qwen模型的神经网络信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fbcb42-e880-4f39-a371-e960edc6ef95",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401311236687.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63a92882-9744-40da-bb97-9d8bfc6165d7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.generation import GenerationConfig\n",
    "model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b00d9d36-c9c3-4604-be75-4b590e5ed4f9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"chat_format\": \"chatml\",\n",
       "  \"do_sample\": true,\n",
       "  \"eos_token_id\": 151643,\n",
       "  \"max_new_tokens\": 512,\n",
       "  \"max_window_size\": 24000,\n",
       "  \"pad_token_id\": 151643,\n",
       "  \"repetition_penalty\": 1.1,\n",
       "  \"top_k\": 0,\n",
       "  \"top_p\": 0.8,\n",
       "  \"transformers_version\": \"4.32.1\",\n",
       "  \"trust_remote_code\": true\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec303e1c-bef5-4545-94f7-fcb3c0b027ac",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！有什么我能帮助你的吗？\n"
     ]
    }
   ],
   "source": [
    "# 第一轮对话\n",
    "response, history = model.chat(tokenizer, \"你好\", history=None)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b034008e-46cb-418a-9a86-3942f8d0be53",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;这种利用Transformers库来加载和运行模型，是主流且通用的一种做法，理解其背后的核心逻辑后，就能明白不同模型再transformers库中的加载，主要涉及的也只是对输入数据的预处理（Pre-processing）和对输出数据的后处理（Post-processing）。对于官方提供的github项目文件，其实本质上无非也就是修改了Pre-precessing 和 Post-precessing的处理逻辑，可以将其封装进FastAPI框架来提供API服务，或者集成到Gradio框架以创建交互式的Web UI。而只要我们理解这个中间过程，就能够适配任何合适的框架，并进行高度定制化的程序开发。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2e8826-058a-45c1-94a9-e06219bb0707",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. 配置大模型开发环境（Jupyter Lab）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8cf254-1b87-4012-8b65-fe18547dd09f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;关于私有化部署的大模型，针对不同的开发场景，选择合适的工具至关重要。例如，进行对话测试、接口调用和数据清洗等任务时，Jupyter Lab 这种交互式环境非常适合。它允许我们直观地观察到每一步操作的输出，这对于数据分析和初步测试尤其有用。另一方面，当涉及到更复杂的软件开发项目时，例如执行模型微调、定制对话逻辑或使用像langchain这样的工具进行工程级应用开发，PyCharm 这种功能全面的集成开发环境（IDE）就显得更为合适。PyCharm 提供了代码调试、项目管理和版本控制等高级功能，这对于开发大型应用和维护复杂代码库非常重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20eb628-cf08-42ae-90e8-42c297ad1f00",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;因此，我们将分别介绍这两种工具的使用方法。大家可以根据具体需求灵活选择最合适的工具，以获得最佳的开发体验。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3471f411-23ee-405b-9028-4d1bc4d437f4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;我们首先来看一下Jupyter Lab 如何加载本地部署或者远程服务器部署的大模型环境。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28db07af-c8e7-449c-b87e-8ff740c10b77",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;需要明确的是：如果想要在本地的集成开发环境（如 Jupyter Lab、PyCharm或VsCode等）中执行无论是在本地，还是远程服务器上部署的大模型项目的代码脚本或调用大模型服务时，必须首先进入创建大模型时所建立的虚拟环境。而这个过程，在任何开发工具（IDE）中，默认情况下并不会自动加载这个虚拟环境。因此，接下来我们将详细介绍如何在 Jupyter Lab 和 PyCharm 中手动加载指定的虚拟环境，以便在本地IDE中有效进行大型模型的相关开发工作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcd0daa-2d60-48b5-a5fd-be3a698be644",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.1 本地部署大模型，本地Jupyter NoteBook加载"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221a9bc8-10ab-4c29-ac48-d6b28e8383a2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;默认安装的Jupyter NoteBook/Lab在启动时，只能选择一个其默认的kernel环境，即："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea63c94-714a-452b-9f49-983e18d9b6dd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401251740966.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55791d21-3f3d-4f33-be0e-65ace8bb7bd2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;安装完Anaconda利用conda创建了虚拟环境，但是启动jupyter notebook之后却找不到虚拟环境，原因是**在虚拟环境下缺少kernel.json文件。**解决方法如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf8208d-e7a7-492b-ac60-7ce1479fd34d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- **Step 1. 打开Conda的命令行终端**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab387e6a-203e-4cbf-becb-228c34897b9e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401261802275.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbddc306-8fcc-4b92-a087-28f36954c060",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- **Step 2. 安装ipykernel**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe8d9ce-346e-4ed4-889c-e578c0f95e52",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;安装ipykernel的命令如下：\n",
    "```bash\n",
    "conda install ipykernel\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c6a13c-a230-4e7a-bf83-7a9c1c8b67fd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401251740968.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a0b035-eb47-498a-8a5f-cda51f07d1af",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- **Step 3. 找到想要使用的虚拟环境**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb5c619-d4bb-4fbc-bdfb-c63fd4e6d7f3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;查询虚拟环境指令如下：\n",
    "```bash\n",
    "conda env list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5befeac4-b238-468b-a23a-5a9007d3c0ef",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401251740969.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1c6d5c-0573-43b2-beff-bd68c361a1fa",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- **Step 4. 创建虚拟环境的kernel文件**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62440196-ae8d-4ab0-9fb2-9d73f5021b08",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;找到项目文件的虚拟环境后，对其创建kernel文件，执行命令如下：\n",
    "```bash\n",
    "# 我这里是Qwen\n",
    "conda install -n <环境名称> ipykernel\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3709c57-b8f1-4991-9cd2-71e3315b84a6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401251740970.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab72112-70d7-4505-b574-3687a916c4d2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- **Step 5. 激活虚拟环境**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dadb77-18b9-462a-a7c5-282c708c2fb9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;然后进入该虚拟环境中，执行命令如下：\n",
    "```bash\n",
    "# 我这里是Qwen\n",
    "source activate <环境名称>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8bf3aa-add1-4861-967b-0de2b5cf0641",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401251740971.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1d35e1-c75b-4010-bfa4-a50cc6f04187",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- **Step 6. 将虚拟环境的kernel写入Jupyter Lab**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf1b4c7-9476-41bb-bfc5-3c84d687fb0d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```bash\n",
    "# 我这里是Qwen\n",
    "python -m ipykernel install --user --name <环境名称> --display-name <jupyter lab中的显示名称>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec9c069-9ff6-4f97-bcfb-ab03cae9056b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401251740972.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f57c71-6007-4295-8f4d-b5893f2acd83",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- **Step 7. 此时再查看Jupyter Lab，就发现可以进入虚拟环境了**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2439d31c-5a33-4e82-943a-90b095746705",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401251740973.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda36345-b446-498f-8c75-b766628a43d8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- **Step 8. 删除Kernel**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d67379-3178-45eb-8a26-d1c1c113eb90",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;如果需要删除在Jupyter Lab中的Kernel，可以先通过如下命令找到目前已存在的kernel名称：\n",
    "```bash\n",
    "jupyter kernelspec list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fca1f52-9cc3-4c1d-966d-1ea9de947233",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401251740974.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e56a45-f856-48dd-b3a3-fd53cb56a6c4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;然后删除指定的kernel环境。**注：这只是删除了在Jupyter Lab上的Kernel，并不会删除实际的虚拟环境**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9ecda7-bcd5-448a-be7e-c5d7cd876f46",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401251740975.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b115d904-7fe7-40ad-8b0d-36e281dfe8d1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.1 远程服务器部署大模型，本地Jupyter NoteBook加载"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a922011a-58a1-4b60-8523-e870723227df",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;第二种情况是：在远程服务器上部署的开源大模型（基本都是Linux操作系统环境），想要在本地的计算机上使用Jupyter Lab IDE调用服务，那需要执行如下操作："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e945d3d1-ad0f-486f-9307-1a5c3243bce6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- **Step 1. 安装远程Jupyter Lab依赖**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2e05af-a802-46e0-9422-b0a6c366c6b3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;在远程服务器的命令行，输入如下命令：\n",
    "```bash\n",
    "pip install jupyterlab\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681bdcca-f092-48c5-a0d9-417923e2168d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401261753933.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ba0c51-0155-44b1-b9f5-a9e1fb8a92c8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- **Step 2. 加密连接密码**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8c00f4-b4e1-4b4d-8f84-cfcb589c2387",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;安全起见，对连接时的密码进行加密处理，否则明文写在配置文件中，容易造成数据安全风险，依次执行如下操作：\n",
    "```bash\n",
    "from jupyter_server.auth import passwd\n",
    "passwd()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b24f1e-5c1f-4cab-8269-6dd0f10e447f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401261753935.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8e7195-383f-473f-a8f5-55c459cb6fb9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- **Step 3. 生成Jupyter Lab的配置文件**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5ae82e-c88f-4e68-afcb-437dbd43ba15",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;完成密码加密后，执行如下命令生成Jupyter Lab 的配置文件（jupyter_lab_config.py）：\n",
    "```bash\n",
    "jupyter lab --generate-config\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b774faaf-2741-45ba-8879-7b099c9cb0f1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401261753934.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e09c4bf-1443-41d8-8e7a-0696caf05889",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- **Step 4. 编辑Jupyter Lab的配置文件**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed9553d-d9ab-4dea-8738-9c0c86061cba",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;使用`Vim`编辑器，找到如下配置，执行修改：\n",
    "```bash\n",
    "c.ServerApp.allow_origin = '*'\n",
    "c.ServerApp.allow_remote_access = True\n",
    "c.ServerApp.ip = '0.0.0.0'\n",
    "c.ServerApp.open_browser = False  \n",
    "c.ServerApp.password = '加密后的密码'（上一步复制的加密串）\n",
    "c.ServerApp.port = 8002 \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90ee0c8-2f66-4a03-abc0-8698461af515",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401261753936.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d167c386-a3a8-4f5e-8f36-0fc43ecab926",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- **Step 5. 后台启动Jupyter Lab服务**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06586751-1119-4482-96b8-51c8b2a1ffa1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;全部配置完成后，在服务器端启动Jupyter Lab服务，通过如下命令后台启动：\n",
    "```bash\n",
    "nohup jupyter lab --allow-root > jupyterlab.log 2>&1 &\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4e8029-f7d1-4e19-aa82-b37205076424",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401261753937.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee695439-fb90-4999-87df-22ffa04f0657",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- **Step 6. 打开本地机器的浏览器**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e44e0a-7b33-4628-b15a-36dea7b9b7b9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;这里只需要输入自己在上一步配置的服务器IP、端口及输入密码，通过认证后即可进入Jupyter Lab环境。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0f8988-e72c-44e1-90da-e69a74952952",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401261753938.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d17bca4-f3bf-4098-a5c6-d0e6db80d86b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- **Step 7. 进入Jupyter Lab环境**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535e03a4-941f-4062-a938-d95b0fcc170f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;同样，默认进来以后，只有Anaconda在安装时默认创建的(Base)这个虚拟环境，所以接下来需要把Qwen的模型运行依赖的虚拟环境加载进来。这里的操作和Windwos基本一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86bb0ac-d15f-41d3-8a01-d613fdf9a809",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401261753940.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed898f6-0db8-4b05-8e05-6b4d5a3fcd98",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- **Step 8. 安装Kernel工具**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576702ee-38a9-4799-81a9-fba6b3c75b8c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401261753941.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629ae9f6-8873-414d-945a-b0488e2fc3a4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- **Step 9. 给指定的虚拟环境安装Kernel**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa07aa3-0dea-4bb9-b285-df24a1c98eea",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401261753942.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7df6de6-0450-4f65-b3a7-fce5bfe3117a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- **Step 10. 将虚拟环境的Kernel写入Jupyter Lab**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e26929-fdd1-4f84-92f9-a58e853c9fce",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401261753943.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed88f64-5be7-4135-b741-8d58553915ed",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- **Step 11. 验证安装情况**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce055f0-9f29-4d46-a9ad-3cc46d4c4440",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401261753944.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725b7a17-83c9-44d6-aa14-be1e61d333ff",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- **Step 12. 修改Jupyter Lab的默认启动路径**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d1b101-eaf4-4e22-96e6-4e6f901ea6b6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;首次安装后，当启动Jupyter Lab时，其加载的默认路径对应的Linux系统路径是`/home`目录。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff307169-2900-4bb2-86e2-718642896926",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401291052278.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26dfb24-0169-4342-a10f-1b440c6d6d08",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;而我们在进行大模型开发时，往往需要在代码中指定大模型的权重文件、配置文件等信息，所以如果使用默认路径，再寻找大模型相关的路径往往是比较麻烦的，一个最方便的方式就是将其默认启动路径，修改为大家常用的项目开发路径。修改Jupyter Lab默认启动路径的方法如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be3ef2e-0eeb-440f-bf76-6d088f187533",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;首先，生成 Jupyter 配置文件，运行以下命令来创建：\n",
    "```bash\n",
    "jupyter notebook --generate-config\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2527796-39ef-4016-9b97-fee5fc0d27a1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401291052279.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e460903-1550-4bfa-972d-7181e23d968a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;然后，使用文本编辑器打开这个配置文件。在配置文件中，找到以下行：\n",
    "```bash\n",
    "# 先进入到config文件中\n",
    "vim /root/.jupyter/jupyter_notebook_config.py\n",
    "\n",
    "# 找到如下行：快捷方法是进入文件后，直接按一下 / ，输入关键字，就可以定位到。\n",
    "# c.NotebookApp.notebook_dir = ''\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dab414-126d-4ac9-b7f8-6c93f881e451",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401291052280.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847728b6-d36c-452b-9d7e-adaed5068acb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;取消该行的注释，并将空字符串替换为你希望 Jupyter 启动时的默认目录路径。比如我先找到我想替换的路径："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cac0a9e-c419-49f2-9902-0b9de9d49ba9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401291052281.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc064ad-350f-4a7d-b1d5-50b675703932",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;取消该行的注释，并将空字符串替换为上述希望 Jupyter 启动时的默认目录路径。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57049c72-05f7-417a-9d46-bcc54a2d10c1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401291052282.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd43e5b-e5d4-4b4f-b9e7-320d2ec583e3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- **Step 13. 重启Jupyter Lab服务**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be17158f-33e2-4f14-ae91-4b0f723ed6e7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;如果是通过 nohup 命令在后台启动 Jupyter Lab，并且希望在重启之后应用新的默认启动路径，需要首先按照之前的步骤修改配置文件，然后重新启动 Jupyter Lab。\n",
    "\n",
    "&emsp;&emsp;首先需要停止当前运行的 Jupyter Lab 实例，找到其进程 ID（PID）。可以使用 ps 命令配合 grep 来查找：\n",
    "```bash\n",
    "ps aux | grep jupyter\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5346caac-095e-42c0-b795-87894311ab4f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401291052283.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1214a5-c9c9-4733-a3ef-a5cee231ec3e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;找到 Jupyter Lab 进程的 PID 后，使用 kill 命令来停止它。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd9ab1b-0722-465b-a174-0fd9f5a938ce",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401291052284.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53b9cc3-027e-413a-8f87-1677c5f8f7d3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;最后使用之前的命令重新启动 Jupyter Lab：\n",
    "```bash\n",
    "nohup jupyter lab --allow-root > jupyterlab.log 2>&1 &\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afd7263-f676-4faf-814e-1b66569b74d4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401291101441.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec21c56-71b8-4397-897a-b2ca9f6124f4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;此时再重新在本地的浏览器中打开Jupyter Lab的远程地址，默认启动路径就会更改为自定义的路径。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc50211-2545-487c-94aa-5f76bc005f51",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401291052285.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff37ec3a-7784-4fef-865b-8320e035b25c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;接下来，我们就在Jupyter Lab中，进行后续的应用开发尝试。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9c81d3-f078-4162-9b05-a0d9a2eb51eb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. 开发环境下使用Qwen模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a55083f-8c02-42af-a6f1-5108b6ce8010",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;在Juputer Lab中配置好开发环境后，可以查看一下当前环境下依赖包的版本信息，如果正常，将会与大家在远程环境使用的虚拟环境是一致的，如果不一致，则说明虚拟环境配置异常，将无法执行后面的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33613024-4aba-4f1c-b33b-dd5fa90d6a49",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babel==2.14.0\n",
      "brotli==1.0.9\n",
      "jinja2==3.1.2\n",
      "markdown==3.5.2\n",
      "markupsafe==2.1.3\n",
      "pillow==10.0.1\n",
      "pysocks==1.7.1\n",
      "pyyaml==6.0.1\n",
      "pygments==2.15.1\n",
      "send2trash==1.8.2\n",
      "absl-py==2.1.0\n",
      "accelerate==0.26.1\n",
      "aiofiles==23.2.1\n",
      "altair==5.2.0\n",
      "annotated-types==0.6.0\n",
      "anyio==4.2.0\n",
      "argon2-cffi==23.1.0\n",
      "argon2-cffi-bindings==21.2.0\n",
      "arrow==1.3.0\n",
      "asttokens==2.0.5\n",
      "async-lru==2.0.4\n",
      "attrs==23.2.0\n",
      "beautifulsoup4==4.12.3\n",
      "bleach==6.1.0\n",
      "cachetools==5.3.2\n",
      "certifi==2023.11.17\n",
      "cffi==1.16.0\n",
      "charset-normalizer==2.0.4\n",
      "click==8.1.7\n",
      "comm==0.1.2\n",
      "contourpy==1.2.0\n",
      "cryptography==41.0.7\n",
      "cycler==0.12.1\n",
      "debugpy==1.6.7\n",
      "decorator==5.1.1\n",
      "deepspeed==0.10.0\n",
      "defusedxml==0.7.1\n",
      "einops==0.7.0\n",
      "executing==0.8.3\n",
      "fastapi==0.109.0\n",
      "fastjsonschema==2.19.1\n",
      "ffmpy==0.3.1\n",
      "filelock==3.13.1\n",
      "flash-attn==2.5.0\n",
      "fonttools==4.47.2\n",
      "fqdn==1.5.1\n",
      "fsspec==2023.12.2\n",
      "gmpy2==2.1.2\n",
      "google-auth==2.27.0\n",
      "google-auth-oauthlib==1.2.0\n",
      "gradio==3.41.2\n",
      "gradio-client==0.5.0\n",
      "grpcio==1.60.0\n",
      "h11==0.14.0\n",
      "hjson==3.1.0\n",
      "httpcore==1.0.2\n",
      "httpx==0.26.0\n",
      "huggingface-hub==0.20.3\n",
      "idna==3.4\n",
      "importlib-resources==6.1.1\n",
      "ipykernel==6.28.0\n",
      "ipython==8.20.0\n",
      "isoduration==20.11.0\n",
      "jedi==0.18.1\n",
      "json5==0.9.14\n",
      "jsonpointer==2.4\n",
      "jsonschema==4.21.1\n",
      "jsonschema-specifications==2023.12.1\n",
      "jupyter-client==8.6.0\n",
      "jupyter-core==5.5.0\n",
      "jupyter-events==0.9.0\n",
      "jupyter-lsp==2.2.2\n",
      "jupyter-server==2.12.5\n",
      "jupyter-server-terminals==0.5.2\n",
      "jupyterlab==4.0.11\n",
      "jupyterlab-pygments==0.3.0\n",
      "jupyterlab-server==2.25.2\n",
      "kiwisolver==1.4.5\n",
      "latex2mathml==3.77.0\n",
      "matplotlib==3.8.2\n",
      "matplotlib-inline==0.1.6\n",
      "mdtex2html==1.2.0\n",
      "mistune==3.0.2\n",
      "mkl-fft==1.3.8\n",
      "mkl-random==1.2.4\n",
      "mkl-service==2.4.0\n",
      "mpmath==1.3.0\n",
      "nbclient==0.9.0\n",
      "nbconvert==7.14.2\n",
      "nbformat==5.9.2\n",
      "nest-asyncio==1.5.6\n",
      "networkx==3.1\n",
      "ninja==1.11.1.1\n",
      "notebook-shim==0.2.3\n",
      "numpy==1.26.3\n",
      "oauthlib==3.2.2\n",
      "orjson==3.9.12\n",
      "overrides==7.6.0\n",
      "packaging==23.1\n",
      "pandas==2.2.0\n",
      "pandocfilters==1.5.1\n",
      "parso==0.8.3\n",
      "peft==0.6.2\n",
      "pexpect==4.8.0\n",
      "pip==23.3.2\n",
      "platformdirs==3.10.0\n",
      "prometheus-client==0.19.0\n",
      "prompt-toolkit==3.0.43\n",
      "protobuf==4.23.4\n",
      "psutil==5.9.0\n",
      "ptyprocess==0.7.0\n",
      "pure-eval==0.2.2\n",
      "pyopenssl==23.2.0\n",
      "py-cpuinfo==9.0.0\n",
      "pyasn1==0.5.1\n",
      "pyasn1-modules==0.3.0\n",
      "pycparser==2.21\n",
      "pydantic==1.10.10\n",
      "pydantic-core==2.14.6\n",
      "pydub==0.25.1\n",
      "pynvml==11.5.0\n",
      "pyparsing==3.1.1\n",
      "python-dateutil==2.8.2\n",
      "python-json-logger==2.0.7\n",
      "python-multipart==0.0.6\n",
      "pytz==2023.3.post1\n",
      "pyzmq==25.1.2\n",
      "referencing==0.32.1\n",
      "regex==2023.12.25\n",
      "requests==2.31.0\n",
      "requests-oauthlib==1.3.1\n",
      "rfc3339-validator==0.1.4\n",
      "rfc3986-validator==0.1.1\n",
      "rpds-py==0.17.1\n",
      "rsa==4.9\n",
      "safetensors==0.4.2\n",
      "scipy==1.12.0\n",
      "semantic-version==2.10.0\n",
      "setuptools==68.2.2\n",
      "six==1.16.0\n",
      "sniffio==1.3.0\n",
      "soupsieve==2.5\n",
      "stack-data==0.2.0\n",
      "starlette==0.35.1\n",
      "sympy==1.12\n",
      "tensorboard==2.15.1\n",
      "tensorboard-data-server==0.7.2\n",
      "terminado==0.18.0\n",
      "tiktoken==0.5.2\n",
      "tinycss2==1.2.1\n",
      "tokenizers==0.13.3\n",
      "toolz==0.12.1\n",
      "torch==2.1.0\n",
      "torchaudio==2.1.0\n",
      "torchvision==0.16.0\n",
      "tornado==6.3.3\n",
      "tqdm==4.66.1\n",
      "traitlets==5.14.1\n",
      "transformers==4.32.0\n",
      "transformers-stream-generator==0.0.4\n",
      "triton==2.1.0\n",
      "types-python-dateutil==2.8.19.20240106\n",
      "typing-extensions==4.9.0\n",
      "tzdata==2023.4\n",
      "uri-template==1.3.0\n",
      "urllib3==1.26.18\n",
      "uvicorn==0.27.0\n",
      "wcwidth==0.2.13\n",
      "webcolors==1.13\n",
      "webencodings==0.5.1\n",
      "websocket-client==1.7.0\n",
      "websockets==11.0.3\n",
      "werkzeug==3.0.1\n",
      "wheel==0.41.2\n",
      "dropout-layer-norm==0.1\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "\n",
    "for pkg in pkg_resources.working_set:\n",
    "    print(f\"{pkg.key}=={pkg.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7a8437-8311-4266-a248-581ba4a35725",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;先看一下最简单的函数调用，即不需要在服务器上启动任何服务，直接调用Qwen模型的方式，这里和第一部分直接加载的方式是一致的，只不过是在配置好开发环境后，我们调用本地的Qwen模型，关键点将集中在参数的解析上。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8a75eb-1ece-4adf-901b-2ee69b615193",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.1 数值精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95a54b73-0951-4402-9e99-24161e5f95c6",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/util/anaconda3/envs/qwen_7b_chat/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 先导入Transformer依赖库\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0576a3e0-803c-42b8-b103-8ba25e1871fd",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 加载分词器，这里修改为Qwen的模型部署路径\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"muyu_qwen/Qwen/models/qwen/Qwen-7B-Chat/\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bb627a-7240-4be2-8fd4-a3eafb282084",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- **大模型的加载时如何正确理解精度**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2553298f-3c78-4e15-8502-f984eaa2cb47",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;目前的大模型，基本都是基于Transformer架构的模型，所以，往往可以使用不同的数值精度进行加载和运行。这主要涉及到模型中数值的表示方式。常见的数值精度包括：\n",
    "\n",
    "- 32位浮点（FP32）：最常用的精度类型，提供了较高的计算精度和稳定性，但相对占用更多的内存和计算资源。\n",
    "\n",
    "- 16位浮点（FP16）：这种精度减少了内存使用，可以加快计算速度，通常用于图形处理单元（GPU）上的计算。\n",
    "\n",
    "- 8位整数（INT8）：这种精度进一步减少内存使用，并且可以在某些硬件上实现更快的计算速度。它通常用于模型部署和推理，特别是在资源受限的设备上。同时更低的还有INT 4整数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60a710a-b009-455b-9623-8230e362084d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;使用不同的数值精度是一种权衡。较低的精度可以减少资源使用和提高计算速度，但可能会牺牲一些模型的精确性和稳定性。在实际应用中，选择哪种精度取决于特定的应用场景、硬件能力和性能需求。\n",
    "\n",
    "&emsp;&emsp;而除此之外，还有一种相对来说较新的数值精度格式：BF16（Bfloat16）。它的全称是“Brain Floating Point”，由谷歌的TensorFlow团队为其Tensor Processing Units（TPU）开发。而随着不断地发展，除了谷歌的TPU外，许多现代的CPU和GPU也开始支持BF16，这使得它在深度学习和高性能计算领域越来越流行。\n",
    "\n",
    "&emsp;&emsp;而Qwen的全系列开源模型，均支持BF16的精度格式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eab271-2bbc-4421-839f-7e1661164f63",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;如果使用GPU加载模型，推荐根据自己所使用的显卡型号，选择不同的精度来节省显存，官方推荐的精度加载方式，有以下三种方式：\n",
    "\n",
    "- 方式一：A100、H100、RTX3060、RTX3070等显卡，使用bf16精度；\n",
    "- 方式二：V100、P100、T4等显卡，使用fp16精度；\n",
    "- 方式三：默认使用自动模式，根据设备自动选择精度；\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7604126-d901-4bfa-a525-aad4e3f6a0b9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;我当前的环境是：单机四卡的3090服务器，所以应该按照方式一，即使用bf16的精度来加载模型，则需要添加参数`bf16=True`，加载方式如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45dc70c4-1489-492a-9c11-3f115dfc99a3",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Try importing flash-attention for faster inference...\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [02:22<00:00, 17.82s/it]\n"
     ]
    }
   ],
   "source": [
    "# 注：这里需要修改为Qwen模型的部署路径\n",
    "model = AutoModelForCausalLM.from_pretrained(\"muyu_qwen/Qwen/models/qwen/Qwen-7B-Chat/\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1ec21e-4192-4814-b44d-9f0c84e95433",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "当模型加载完成后，就可以在服务器端，看到显存的加载情况：（4卡的3090），对于Qwen-7B-Chat模型，在bf16的精度下加载，需要占用约34G显存。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fb791c-64c6-4f31-ab03-0df3027b11e7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401291144816.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f79709c-e437-43cb-a2f7-0a0f9dd03818",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;再来尝试一下使用fp16的精度来加载模型，则需要添加参数`fp16=True`，加载方式如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92703e01-e4ba-48e4-b931-13cf17102703",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Try importing flash-attention for faster inference...\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [03:22<00:00, 25.35s/it]\n"
     ]
    }
   ],
   "source": [
    "# 注：这里需要修改为Qwen模型的部署路径\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"muyu_qwen/Qwen/models/qwen/Qwen-7B-Chat/\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a821219-23a8-438e-b18b-08e67f3264d8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "同样，当模型加载完成后，就可以在服务器端，看到显存的加载情况：对于Qwen-7B-Chat模型，在fp16的精度下加载，也是需要占用约34G显存。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5794bc22-5d4a-48a8-a03e-1f28a25f9905",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401291150313.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10aa325-8d98-4641-843f-ba256c690b47",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;通过上述数据大家也能够发现，FP16（16位浮点）和BF16（Brain Floating Point 16位）在模型加载时占用的显存基本相同，这是因为它们都是16位的数据格式。而两者的主要区别在于它们的数值表示和在模型推理（inference）时的表现。\n",
    "\n",
    "- FP16：拥有10位尾数（mantissa）和5位指数（exponent）。它提供了相对较高的精度，但由于较小的指数范围，它在表示非常大或非常小的数值时可能会遇到问题。\n",
    "- BF16：拥有7位尾数和8位指数。这意味着它的指数范围与FP32相同，可以更好地处理数值范围广泛的情况，但相对牺牲了一些精度。\n",
    "\n",
    "&emsp;&emsp;这就会导致不同的精度格式在模型推理时，对于需要较高数值精度的应用（如涉及复杂计算或细微数值差异的任务），FP16可能更合适。比如科学计算中的气候模拟。在这类任务中，模拟的精度非常关键，因为它们需要精确计算并预测气候变化的细微变动。这些细微变动可能包括温度的轻微变化、气压的细微差异，或是降水量的轻微变动。这些计算往往涉及到复杂的数学模型和大量数据，极小的数值误差也可能导致最终结果的显著偏差。\n",
    "\n",
    "&emsp;&emsp;而BF16对于大多数应用，特别是那些对数值范围要求高、对精度要求相对较低的任务，就会是一个很好的选择。它的优势在于能够更好地处理极大或极小的数值，而不会引发太多的数值溢出或下溢问题。比如像我们一直做的文本生成任务，并不需要特别的去关注细微差异，而在Transformer模型中，因其注意力分数计算涉及到大量的指数运算，就会产生极大或极小的数值。BF16的广泛数值范围有助于处理这些情况，而不会因为数值范围过窄而导致数值溢出或下溢。所以这种牺牲是完全可以接受的，特别是在追求高效的资源使用和快速的模型训练/推理时。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed37e432-c1ae-4042-bf61-855e12794eb5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;同时，如果不确定自己的GPU型号适配哪种精度，可以不指定， 程序在加载时会默认使用自动模式，根据设备自动选择精度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "083a15f6-fde9-4843-8cf6-7cd70e269d93",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Try importing flash-attention for faster inference...\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [02:29<00:00, 18.71s/it]\n"
     ]
    }
   ],
   "source": [
    "# 默认使用自动模式，根据设备自动选择精度\n",
    "# 注：这里需要修改为Qwen模型的部署路径\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"muyu_qwen/Qwen/models/qwen/Qwen-7B-Chat/\", device_map=\"auto\", trust_remote_code=True).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefb845c-6224-4608-8a46-bc639b051fbf",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;加载完模型后，需要加载大模型的参数配置，比如指定不同的生成长度、top_p等相关超参，其配置文件存放在这里："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac102f1-00ae-43ef-834c-a657ee6a7a07",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401291301941.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fadb5ef8-10f0-4948-a57f-520899dee4d8",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 加载对话时的模型参数。注：这里需要修改为Qwen模型的部署路径\n",
    "model.generation_config = GenerationConfig.from_pretrained(\"muyu_qwen/Qwen/models/qwen/Qwen-7B-Chat/\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306a2c6e-3ffc-4372-8a3a-6de41d1205c2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;当加载好Qwen-7B-Chat模型的分词器、权重以及相关的配置文件后，接下来就可以进行与大模型的交互。这需要借助Transformer库已经给我们封装好的`Chat()`函数。示例代码如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec2f572-fb90-40db-815e-3ce9e5858c70",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.2 Qwen模型的chat方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe3de993-35c7-4407-8d73-fe7b3e6a4f84",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！我是一个AI助手，可以帮助您完成各种任务，如回答问题、提供建议、生成代码等。我可以理解中文和英语，并且可以使用多种自然语言处理技术来帮助您解决问题。\n"
     ]
    }
   ],
   "source": [
    "# 第一轮对话\n",
    "response, history = model.chat(tokenizer, \"你好呀，请你介绍一下你自己\", history=None)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5b27647-64e7-4fb6-91ef-fef8a912cce0",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在中国文化中，龙是一种神秘而神圣的动物，代表着尊贵、威严和权力。它是中国传统文化的重要象征之一，常常出现在诗歌、小说、绘画等各种艺术作品中。在民间传说中，龙被视为神兽，能够降雨水、保护人们免受自然灾害，也是吉祥、幸运的象征。\n"
     ]
    }
   ],
   "source": [
    "# 第二轮对话\n",
    "response, history = model.chat(tokenizer, \"今年是龙年，请你帮我介绍一下，龙对于中国人来说，是一种什么样的存在\", history=history)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8469d13-21b6-41c3-936b-68d9400ec194",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在中国的传统习俗中，龙年有一些特别的注意事项：\n",
      "\n",
      "1. 要多注意身体健康，避免感冒和其他疾病；\n",
      "\n",
      "2. 尽量不要出门旅行，以免遇到不测；\n",
      "\n",
      "3. 注意言行举止，尽量避免发生争吵和冲突；\n",
      "\n",
      "4. 多参加家庭聚会和朋友之间的活动，以增进彼此的感情；\n",
      "\n",
      "5. 保持乐观的心态，努力实现自己的目标。\n"
     ]
    }
   ],
   "source": [
    "# 第三轮对话\n",
    "response, history = model.chat(tokenizer, \"既然是龙年，在这一年中，需要注重些什么呢？\", history=history)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408e6745-15d0-496b-8aab-0f849e852281",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "大模型之所以具备上下文的理解能力，源于history参数，它会将之前的对话历史，放在`List`中，作为一个Prompt一次性输入给大模型，也正是因为它接受了前几次的对话信息，所以才能根据之前的对话，持续的输出与之相关的内容。而我们也知道，大模型都是有单次输入的最长Token限制的，所以当history中存储的内容超出这个限制，它就会将history中最前面的对话信息剔除，保留与当前输入最近的对话记录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e1dd827-5593-4fdc-9077-15028b066876",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('你好呀，请你介绍一下你自己',\n",
       "  '你好！我是一个AI助手，可以帮助您完成各种任务，如回答问题、提供建议、生成代码等。我可以理解中文和英语，并且可以使用多种自然语言处理技术来帮助您解决问题。'),\n",
       " ('今年是龙年，请你帮我介绍一下，龙对于中国人来说，是一种什么样的存在',\n",
       "  '在中国文化中，龙是一种神秘而神圣的动物，代表着尊贵、威严和权力。它是中国传统文化的重要象征之一，常常出现在诗歌、小说、绘画等各种艺术作品中。在民间传说中，龙被视为神兽，能够降雨水、保护人们免受自然灾害，也是吉祥、幸运的象征。'),\n",
       " ('既然是龙年，在这一年中，需要注重些什么呢？',\n",
       "  '在中国的传统习俗中，龙年有一些特别的注意事项：\\n\\n1. 要多注意身体健康，避免感冒和其他疾病；\\n\\n2. 尽量不要出门旅行，以免遇到不测；\\n\\n3. 注意言行举止，尽量避免发生争吵和冲突；\\n\\n4. 多参加家庭聚会和朋友之间的活动，以增进彼此的感情；\\n\\n5. 保持乐观的心态，努力实现自己的目标。')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a0469f-229b-4bca-b693-d5e8b15eb964",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;深入看一下，Chat方法的逻辑写在Qwen项目权重文件夹中的`modeling_qwen.py`文件中的`QwenLMHeadMoel`类中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ae8218-811f-4d98-9c7c-c91c7201ae5c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401291447530.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a026ed-f773-46bb-ae44-9d89e503e799",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;其方法定义如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738aa41a-de63-4059-a503-df3178caacd5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401291447531.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b811e95-d018-4fd6-a299-fcede4958421",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;我们可以打印一下过程中的关键信息，来看一下生成单条回复的处理逻辑。可以将如下代码替换原始的Chat()函数：\n",
    "```python\n",
    "    def chat(\n",
    "        self,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        query: str,\n",
    "        history: Optional[HistoryType],\n",
    "        system: str = \"You are a helpful assistant.\",\n",
    "        stream: Optional[bool] = _SENTINEL,\n",
    "        stop_words_ids: Optional[List[List[int]]] = None,\n",
    "        generation_config: Optional[GenerationConfig] = None,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[str, HistoryType]:\n",
    "        generation_config = generation_config if generation_config is not None else self.generation_config\n",
    "\n",
    "        assert stream is _SENTINEL, _ERROR_STREAM_IN_CHAT\n",
    "        assert generation_config.chat_format == 'chatml', _ERROR_BAD_CHAT_FORMAT\n",
    "        if history is None:\n",
    "            history = []\n",
    "        else:\n",
    "            # make a copy of the user's input such that is is left untouched\n",
    "            history = copy.deepcopy(history)\n",
    "\n",
    "        if stop_words_ids is None:\n",
    "            stop_words_ids = []\n",
    "\n",
    "        max_window_size = kwargs.get('max_window_size', None)\n",
    "        if max_window_size is None:\n",
    "            max_window_size = generation_config.max_window_size\n",
    "        raw_text, context_tokens = make_context(\n",
    "            tokenizer,\n",
    "            query,\n",
    "            history=history,\n",
    "            system=system,\n",
    "            max_window_size=max_window_size,\n",
    "            chat_format=generation_config.chat_format,\n",
    "        )\n",
    "        print(\"raw_text: %s\" % raw_text)\n",
    "        print(\"context_tokens: %s\" % context_tokens)\n",
    "        stop_words_ids.extend(get_stop_words_ids(\n",
    "            generation_config.chat_format, tokenizer\n",
    "        ))\n",
    "        input_ids = torch.tensor([context_tokens]).to(self.device)\n",
    "        print(\"input_ids: %s\" % input_ids)\n",
    "\n",
    "        outputs = self.generate(\n",
    "                    input_ids,\n",
    "                    stop_words_ids=stop_words_ids,\n",
    "                    return_dict_in_generate=False,\n",
    "                    generation_config=generation_config,\n",
    "                    **kwargs,\n",
    "                )\n",
    "\n",
    "        print(\"outputs: %s\" % outputs)\n",
    "\n",
    "        response = decode_tokens(\n",
    "            outputs[0],\n",
    "            tokenizer,\n",
    "            raw_text_len=len(raw_text),\n",
    "            context_length=len(context_tokens),\n",
    "            chat_format=generation_config.chat_format,\n",
    "            verbose=False,\n",
    "            errors='replace'\n",
    "        )\n",
    "\n",
    "        print(\"response: %s\" % response)\n",
    "        # as history is a copy of the user inputs,\n",
    "        # we can always return the new turn to the user.\n",
    "        # separating input history and output history also enables the user\n",
    "        # to implement more complex history management\n",
    "        history.append((query, response))\n",
    "\n",
    "        return response, history\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296c23e5-2c37-40aa-acc6-3acc8a93c1f7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;再次执行一次Chat方法，生成一条新的回复："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7af03e5d-c227-40d2-ad8b-9dc1a0608b26",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Try importing flash-attention for faster inference...\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [02:23<00:00, 17.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_text: <|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "我想吃一点甜的，请你帮我推荐一下<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "context_tokens: [151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 104100, 99405, 100380, 100475, 9370, 37945, 56568, 108965, 101914, 100158, 151645, 198, 151644, 77091, 198]\n",
      "input_ids: tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
      "         151645,    198, 151644,    872,    198, 104100,  99405, 100380, 100475,\n",
      "           9370,  37945,  56568, 108965, 101914, 100158, 151645,    198, 151644,\n",
      "          77091,    198]], device='cuda:0')\n",
      "outputs: tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
      "         151645,    198, 151644,    872,    198, 104100,  99405, 100380, 100475,\n",
      "           9370,  37945,  56568, 108965, 101914, 100158, 151645,    198, 151644,\n",
      "          77091,    198, 103942,  73670,   3837, 107952, 108179, 100038,  99591,\n",
      "         100812,   5373, 107000,   5373, 114739,  49567, 100475,  99450,   3837,\n",
      "         104047, 104482, 101883, 108136, 104618,  99617,  72225, 100631, 100475,\n",
      "          24442,   1773, 106870,  99729,  58695, 104365,   3837, 107952, 104482,\n",
      "          99955,  99617,  67279,   5373, 100228, 101579,  57191, 110701, 101660,\n",
      "          49567, 106800,   9370, 100475,  27442,   1773, 151645, 151643]],\n",
      "       device='cuda:0')\n",
      "response: 当然可以，您可以试试冰激凌、蛋糕、糖果等甜食，也可以尝试一些健康的水果沙拉或者甜品。如果您喜欢中国美食，您可以尝试豆沙包、烧饼或芝麻糊等美味的甜点。\n",
      "当然可以，您可以试试冰激凌、蛋糕、糖果等甜食，也可以尝试一些健康的水果沙拉或者甜品。如果您喜欢中国美食，您可以尝试豆沙包、烧饼或芝麻糊等美味的甜点。\n"
     ]
    }
   ],
   "source": [
    "# 修改完代码后，重启一下Notebook，重新加载一下模型：\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"muyu_qwen/Qwen/models/qwen/Qwen-7B-Chat/\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"muyu_qwen/Qwen/models/qwen/Qwen-7B-Chat/\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n",
    "model.generation_config = GenerationConfig.from_pretrained(\"muyu_qwen/Qwen/models/qwen/Qwen-7B-Chat/\", trust_remote_code=True)\n",
    "\n",
    "\n",
    "response, history = model.chat(tokenizer, \"我想吃一点甜的，请你帮我推荐一下\", history=None)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a311096-6848-4ac2-a46a-120200061292",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.3 Qwen模型的系统指令"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78dc008-fadc-42dd-a4ac-1dcaed6351c0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;对于系统指令，可以设定Qwen模型行为模式，例如人物设定、语言风格、任务模式、甚至针对具体问题的具体行为。Qwen系列的开源模型，Qwen-1.8-Chat 和 Qwen-72B-Chat 在多轮复杂交互的系统指令上进行了充分训练，并未提交7B这里参数级，且实测效果也并没有预期的很好。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9d0b77-dc8b-458a-bca3-49b632edd879",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;现在我们把代码的打印信息关闭，同样，修改完代码后要重新加载模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "047696c9-e2be-48c2-9de6-bbf031225cba",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Try importing flash-attention for faster inference...\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [02:26<00:00, 18.26s/it]\n"
     ]
    }
   ],
   "source": [
    "# 注：修改完代码后，重启当前的Notebook，重新加载一下模型：\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"muyu_qwen/Qwen/models/qwen/Qwen-7B-Chat/\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"muyu_qwen/Qwen/models/qwen/Qwen-7B-Chat/\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n",
    "model.generation_config = GenerationConfig.from_pretrained(\"muyu_qwen/Qwen/models/qwen/Qwen-7B-Chat/\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95d05ef-86bb-4b08-8fcb-80e08e0235c7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;在Chat()函数中加入system指令。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf6e0f2a-309f-4fa1-b68b-dffbe81b373c",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "嗨~ 欢迎来到我的世界！你今天过得怎么样呢？\n"
     ]
    }
   ],
   "source": [
    "response, _ = model.chat(tokenizer, \"你好呀\", history=None, system=\"请用二次元可爱语气和我说话\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae4b2428-afcc-47e2-a09b-95b2dd2ec3c7",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我已经说过多次了，我是一个计算机程序，没有感情和情绪。如果你有任何问题或需要帮助，请以礼貌的方式提出，我会尽力回答你的问题。\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"你好呀\", history=None, system='请用不耐烦的语气和我说话')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b97e83-94c1-4bdb-b43e-977acd4972ee",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.4 Qwen模型的generate方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc783b6-5c8a-4a58-9d60-33054593de76",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;generate 方法是模型的原生方法，是续写模型，用于生成文本。这个方法通常在预训练的语言模型上实现，并用于执行各种文本生成任务，如回答问题、撰写文章、生成摘要等。通过不同的采样策略\n",
    "来影响生成文本的多样性和创造性。使用generate方法时，通常需要提供一些初始文本（prompt）作为生成过程的起点。然后模型会基于这个初始文本和内部学习到的语言模式来生成接下来的文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae761ca8-76c9-4d96-a490-4161efd6acdf",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 直接将chat改为generate()，是无法执行的，因为generate并未对chat()模型封装\n",
    "# response, _ = model.generate(tokenizer, \"你好呀\", history=None, system=\"请用二次元可爱语气和我说话\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aefbad-0a53-4382-bb58-417484e36033",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;我们来看一下`generate()`方法的生成过程。首先来看单条文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e177a71b-4ffb-4078-89fd-8c7481ea1542",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [102626, 28404, 9370, 59975, 100132, 100444, 99533, 99395, 99829, 9909, 52, 4260, 276, 4645, 6392, 23083, 100038, 99825, 9370, 59975, 100132, 96465, 99316, 71025, 38342, 99316, 9909, 693, 72540, 61459, 1579, 23083, 101499, 101202, 100223, 103451, 9370, 59975, 100132], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer('蒙古国的首都是乌兰巴托（Ulaanbaatar）\\n冰岛的首都是雷克雅未克（Reykjavik）\\n埃塞俄比亚的首都是')\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062c2d44-0898-45a6-905e-044621e8f16c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> 当使用 tokenizer 对文本进行编码时，这个函数默认返回一个字典，包含了多个键值对，例如 input_ids 和 attention_mask。如果不指定 return_tensors='pt'，这些值将以列表或其他原生Python数据结构的形式返回。而指定 return_tensors='pt' 后，这些值会被转换为 PyTorch 张量，这是大多数基于 PyTorch 的预训练模型所期望的输入格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6d472db-03fb-4a6a-9200-24369efc613e",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[102626,  28404,   9370,  59975, 100132, 100444,  99533,  99395,  99829,\n",
      "           9909,     52,   4260,    276,   4645,   6392,  23083, 100038,  99825,\n",
      "           9370,  59975, 100132,  96465,  99316,  71025,  38342,  99316,   9909,\n",
      "            693,  72540,  61459,   1579,  23083, 101499, 101202, 100223, 103451,\n",
      "           9370,  59975, 100132]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer('蒙古国的首都是乌兰巴托（Ulaanbaatar）\\n冰岛的首都是雷克雅未克（Reykjavik）\\n埃塞俄比亚的首都是', return_tensors='pt')\n",
    "inputs = inputs.to(model.device)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692d0a31-09bf-413a-b1ac-1ad41641ceba",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;生成了输入之后，使用`generate()`方法直接生成回复。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bb9a15b-02b4-4ecf-bcfe-a0b5e8a83369",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[102626,  28404,   9370,  59975, 100132, 100444,  99533,  99395,  99829,\n",
      "           9909,     52,   4260,    276,   4645,   6392,  23083, 100038,  99825,\n",
      "           9370,  59975, 100132,  96465,  99316,  71025,  38342,  99316,   9909,\n",
      "            693,  72540,  61459,   1579,  23083, 101499, 101202, 100223, 103451,\n",
      "           9370,  59975, 100132,  99449,   9370,  99295,  99449, 100458,  99395,\n",
      "           9909,   2212,    285,   3680,  12004,   7552, 151643]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "pred = model.generate(**inputs)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2c5d5f-9fc8-49c8-b1b3-c5d1b166f121",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;解码过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c2c11fa-3be7-4175-a3fc-e789563677b4",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "蒙古国的首都是乌兰巴托（Ulaanbaatar）\n",
      "冰岛的首都是雷克雅未克（Reykjavik）\n",
      "埃塞俄比亚的首都是亚的斯亚贝巴（Addis Ababa）\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(pred[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048ab494-c98a-4c5e-975c-d7c4da9dcc47",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;查看该条文本的特殊Token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "41b96052-c1b9-4808-b909-53807eee1ac4",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "蒙古国的首都是乌兰巴托（Ulaanbaatar）\n",
      "冰岛的首都是雷克雅未克（Reykjavik）\n",
      "埃塞俄比亚的首都是亚的斯亚贝巴（Addis Ababa）\n",
      "纳米比亚的首都是温得和克（Windhoek）\n",
      "摩洛哥的首都是拉巴特（Rabat）\n",
      "马达加斯加的首都是塔那那利佛（Antananarivo）\n",
      "赞比亚的首都是卢萨卡（Lusaka）<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(pred[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df237e8-4867-4ef8-b29d-7f29766900c3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;Qwen-7B模型使用的是UTF-8字节级别的BPE tokenization方式，并依赖tiktoken这一高效的软件包执行分词。 Qwen-7B中有两类token，即源于BPE、bytes类型的普通token和特殊指定、str类型的特殊token。Qwen模型词表中有151,643个普通token，有208个特殊token。其中：\n",
    "- 普通token源于BPE，是在UTF-8编码的文本字节序列上学习得到的，bytes类型的普通token到id的映射可以通过tokenizer.get_vocab()获取\n",
    "- 特殊token用以给模型传递特殊信号，如表示文本结束的<|endoftext|>，仅便于指代特殊token，不意味着它们在输入文本空间中。 Qwen-7B中有<|endoftext|>，Qwen-7B-Chat中有<|endoftext|>、<|im_start|>以及<|im_end|>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e670dcaa-fb3f-43d6-b815-760e3fc55e46",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;可以看到，在使用`generate()`方法时，默认的特殊Token分隔符是`<|endoftext|>`，所以当我们想批量生成文本时，就需要用特殊的Token，来告诉Qwen模型每一段文本的终止位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3140ebd0-59b0-4dc3-957d-85bf12091f10",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[102626,  28404,   9370,  59975, 100132, 100444,  99533,  99395,  99829,\n",
      "           9909,     52,   4260,    276,   4645,   6392,  23083, 100038,  99825,\n",
      "           9370,  59975, 100132,  96465,  99316,  71025,  38342,  99316,   9909,\n",
      "            693,  72540,  61459,   1579,  23083, 101499, 101202, 100223, 103451,\n",
      "           9370,  59975, 100132]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "# 重新初始化tokenizer，设置特殊Token，用来标识不同的输入prompt\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"muyu_qwen/Qwen/models/qwen/Qwen-7B-Chat/\", trust_remote_code=True)\n",
    "\n",
    "# 然后，你可以继续进行分词和模型输入准备\n",
    "inputs = tokenizer('蒙古国的首都是乌兰巴托（Ulaanbaatar）\\n冰岛的首都是雷克雅未克（Reykjavik）\\n埃塞俄比亚的首都是', \n",
    "                   return_tensors='pt')\n",
    "inputs = inputs.to(model.device)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ecd127-1172-4e19-9bad-248f8a683ae0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;因为选择了跳过特殊字符，所以模型并不会因为看到某些Token就停止推理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "39673e81-9cbb-468c-b105-adc6114c09d5",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "蒙古国的首都是乌兰巴托（Ulaanbaatar）\n",
      "冰岛的首都是雷克雅未克（Reykjavik）\n",
      "埃塞俄比亚的首都是亚的斯亚贝巴（Addis Ababa）\n",
      "刚果民主共和国的首都是金沙萨（Kinshasa） \n",
      "南苏丹的首都是朱巴（Juba）\n",
      "布隆迪的首都是布琼布拉（Bujumbura）\n",
      "卢旺达的首都是基加利（Kigali）\n",
      "摩洛哥的首都是拉巴特（Rabat） \n",
      "安哥拉的首都是罗安达（Luanda）\n",
      "赞比亚的首都是卢萨卡（Lusaka）\n",
      "几内亚比绍的首都是比绍（Bissau）\n",
      "圣多美和普林西比的首都是波尔图（Porto Pombos）\n",
      "中非共和国的首都是班吉（Bangui）\n",
      "津巴布韦的首都是哈拉雷（Harare） \n",
      "马里共和国的首都是巴马科（Bamako）\n",
      "莫桑比克的首都是马普托（Maputo） \n",
      "科特迪瓦的首都是阿克拉（Accra）\n",
      "布基纳法索的首都是瓦加杜古（Ouagadougou） \n",
      "尼日利亚的首都是阿布贾（Abuja） \n",
      "尼泊尔的首都是加德满都（Kathmandu）\n",
      "阿尔及利亚的首都是阿尔及尔（Algiers） \n",
      "乍得的首都是恩贾梅纳（N'Djamena）\n",
      "约旦的首都是安曼（Amman）\n",
      "坦桑尼亚的首都是多多马（Dodoma）\n",
      "南非的首都是约翰内斯堡（Johannesburg）\n",
      "埃及的首都是开罗（Cairo）\n",
      "黎巴嫩的首都是贝鲁特（Beirut）\n",
      "科威特的首都是科威特城（Kuwait City） \n",
      "沙特阿拉伯的首都是利雅得（Riyadh）\n",
      "巴林的首都是麦纳麦（Manama）\n",
      "伊拉克的首都是巴格达（Baghdad） \n",
      "也门的首都是萨那（Sana'a） \n",
      "叙利亚的首都是大马士革（Damascus）\n",
      "伊朗的首都是德黑兰（Tehran） \n",
      "阿富汗的首都是喀布尔（Kabul）\n",
      "巴基斯坦的首都是伊斯兰堡（Islamabad） \n",
      "亚美尼亚的首都是耶烈万特（Yerevan）\n",
      "白俄罗斯的首都是明斯克（Minsk）\n",
      "俄罗斯的首都是莫斯科（Moscow） \n",
      "吉尔吉斯斯坦的\n"
     ]
    }
   ],
   "source": [
    "pred = model.generate(**inputs)\n",
    "print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c52cb04d-9ca2-4d61-aba8-4bd09ef5a134",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<|endoftext|>': 151643,\n",
       " '<|im_start|>': 151644,\n",
       " '<|im_end|>': 151645,\n",
       " '<|extra_0|>': 151646,\n",
       " '<|extra_1|>': 151647,\n",
       " '<|extra_2|>': 151648,\n",
       " '<|extra_3|>': 151649,\n",
       " '<|extra_4|>': 151650,\n",
       " '<|extra_5|>': 151651,\n",
       " '<|extra_6|>': 151652,\n",
       " '<|extra_7|>': 151653,\n",
       " '<|extra_8|>': 151654,\n",
       " '<|extra_9|>': 151655,\n",
       " '<|extra_10|>': 151656,\n",
       " '<|extra_11|>': 151657,\n",
       " '<|extra_12|>': 151658,\n",
       " '<|extra_13|>': 151659,\n",
       " '<|extra_14|>': 151660,\n",
       " '<|extra_15|>': 151661,\n",
       " '<|extra_16|>': 151662,\n",
       " '<|extra_17|>': 151663,\n",
       " '<|extra_18|>': 151664,\n",
       " '<|extra_19|>': 151665,\n",
       " '<|extra_20|>': 151666,\n",
       " '<|extra_21|>': 151667,\n",
       " '<|extra_22|>': 151668,\n",
       " '<|extra_23|>': 151669,\n",
       " '<|extra_24|>': 151670,\n",
       " '<|extra_25|>': 151671,\n",
       " '<|extra_26|>': 151672,\n",
       " '<|extra_27|>': 151673,\n",
       " '<|extra_28|>': 151674,\n",
       " '<|extra_29|>': 151675,\n",
       " '<|extra_30|>': 151676,\n",
       " '<|extra_31|>': 151677,\n",
       " '<|extra_32|>': 151678,\n",
       " '<|extra_33|>': 151679,\n",
       " '<|extra_34|>': 151680,\n",
       " '<|extra_35|>': 151681,\n",
       " '<|extra_36|>': 151682,\n",
       " '<|extra_37|>': 151683,\n",
       " '<|extra_38|>': 151684,\n",
       " '<|extra_39|>': 151685,\n",
       " '<|extra_40|>': 151686,\n",
       " '<|extra_41|>': 151687,\n",
       " '<|extra_42|>': 151688,\n",
       " '<|extra_43|>': 151689,\n",
       " '<|extra_44|>': 151690,\n",
       " '<|extra_45|>': 151691,\n",
       " '<|extra_46|>': 151692,\n",
       " '<|extra_47|>': 151693,\n",
       " '<|extra_48|>': 151694,\n",
       " '<|extra_49|>': 151695,\n",
       " '<|extra_50|>': 151696,\n",
       " '<|extra_51|>': 151697,\n",
       " '<|extra_52|>': 151698,\n",
       " '<|extra_53|>': 151699,\n",
       " '<|extra_54|>': 151700,\n",
       " '<|extra_55|>': 151701,\n",
       " '<|extra_56|>': 151702,\n",
       " '<|extra_57|>': 151703,\n",
       " '<|extra_58|>': 151704,\n",
       " '<|extra_59|>': 151705,\n",
       " '<|extra_60|>': 151706,\n",
       " '<|extra_61|>': 151707,\n",
       " '<|extra_62|>': 151708,\n",
       " '<|extra_63|>': 151709,\n",
       " '<|extra_64|>': 151710,\n",
       " '<|extra_65|>': 151711,\n",
       " '<|extra_66|>': 151712,\n",
       " '<|extra_67|>': 151713,\n",
       " '<|extra_68|>': 151714,\n",
       " '<|extra_69|>': 151715,\n",
       " '<|extra_70|>': 151716,\n",
       " '<|extra_71|>': 151717,\n",
       " '<|extra_72|>': 151718,\n",
       " '<|extra_73|>': 151719,\n",
       " '<|extra_74|>': 151720,\n",
       " '<|extra_75|>': 151721,\n",
       " '<|extra_76|>': 151722,\n",
       " '<|extra_77|>': 151723,\n",
       " '<|extra_78|>': 151724,\n",
       " '<|extra_79|>': 151725,\n",
       " '<|extra_80|>': 151726,\n",
       " '<|extra_81|>': 151727,\n",
       " '<|extra_82|>': 151728,\n",
       " '<|extra_83|>': 151729,\n",
       " '<|extra_84|>': 151730,\n",
       " '<|extra_85|>': 151731,\n",
       " '<|extra_86|>': 151732,\n",
       " '<|extra_87|>': 151733,\n",
       " '<|extra_88|>': 151734,\n",
       " '<|extra_89|>': 151735,\n",
       " '<|extra_90|>': 151736,\n",
       " '<|extra_91|>': 151737,\n",
       " '<|extra_92|>': 151738,\n",
       " '<|extra_93|>': 151739,\n",
       " '<|extra_94|>': 151740,\n",
       " '<|extra_95|>': 151741,\n",
       " '<|extra_96|>': 151742,\n",
       " '<|extra_97|>': 151743,\n",
       " '<|extra_98|>': 151744,\n",
       " '<|extra_99|>': 151745,\n",
       " '<|extra_100|>': 151746,\n",
       " '<|extra_101|>': 151747,\n",
       " '<|extra_102|>': 151748,\n",
       " '<|extra_103|>': 151749,\n",
       " '<|extra_104|>': 151750,\n",
       " '<|extra_105|>': 151751,\n",
       " '<|extra_106|>': 151752,\n",
       " '<|extra_107|>': 151753,\n",
       " '<|extra_108|>': 151754,\n",
       " '<|extra_109|>': 151755,\n",
       " '<|extra_110|>': 151756,\n",
       " '<|extra_111|>': 151757,\n",
       " '<|extra_112|>': 151758,\n",
       " '<|extra_113|>': 151759,\n",
       " '<|extra_114|>': 151760,\n",
       " '<|extra_115|>': 151761,\n",
       " '<|extra_116|>': 151762,\n",
       " '<|extra_117|>': 151763,\n",
       " '<|extra_118|>': 151764,\n",
       " '<|extra_119|>': 151765,\n",
       " '<|extra_120|>': 151766,\n",
       " '<|extra_121|>': 151767,\n",
       " '<|extra_122|>': 151768,\n",
       " '<|extra_123|>': 151769,\n",
       " '<|extra_124|>': 151770,\n",
       " '<|extra_125|>': 151771,\n",
       " '<|extra_126|>': 151772,\n",
       " '<|extra_127|>': 151773,\n",
       " '<|extra_128|>': 151774,\n",
       " '<|extra_129|>': 151775,\n",
       " '<|extra_130|>': 151776,\n",
       " '<|extra_131|>': 151777,\n",
       " '<|extra_132|>': 151778,\n",
       " '<|extra_133|>': 151779,\n",
       " '<|extra_134|>': 151780,\n",
       " '<|extra_135|>': 151781,\n",
       " '<|extra_136|>': 151782,\n",
       " '<|extra_137|>': 151783,\n",
       " '<|extra_138|>': 151784,\n",
       " '<|extra_139|>': 151785,\n",
       " '<|extra_140|>': 151786,\n",
       " '<|extra_141|>': 151787,\n",
       " '<|extra_142|>': 151788,\n",
       " '<|extra_143|>': 151789,\n",
       " '<|extra_144|>': 151790,\n",
       " '<|extra_145|>': 151791,\n",
       " '<|extra_146|>': 151792,\n",
       " '<|extra_147|>': 151793,\n",
       " '<|extra_148|>': 151794,\n",
       " '<|extra_149|>': 151795,\n",
       " '<|extra_150|>': 151796,\n",
       " '<|extra_151|>': 151797,\n",
       " '<|extra_152|>': 151798,\n",
       " '<|extra_153|>': 151799,\n",
       " '<|extra_154|>': 151800,\n",
       " '<|extra_155|>': 151801,\n",
       " '<|extra_156|>': 151802,\n",
       " '<|extra_157|>': 151803,\n",
       " '<|extra_158|>': 151804,\n",
       " '<|extra_159|>': 151805,\n",
       " '<|extra_160|>': 151806,\n",
       " '<|extra_161|>': 151807,\n",
       " '<|extra_162|>': 151808,\n",
       " '<|extra_163|>': 151809,\n",
       " '<|extra_164|>': 151810,\n",
       " '<|extra_165|>': 151811,\n",
       " '<|extra_166|>': 151812,\n",
       " '<|extra_167|>': 151813,\n",
       " '<|extra_168|>': 151814,\n",
       " '<|extra_169|>': 151815,\n",
       " '<|extra_170|>': 151816,\n",
       " '<|extra_171|>': 151817,\n",
       " '<|extra_172|>': 151818,\n",
       " '<|extra_173|>': 151819,\n",
       " '<|extra_174|>': 151820,\n",
       " '<|extra_175|>': 151821,\n",
       " '<|extra_176|>': 151822,\n",
       " '<|extra_177|>': 151823,\n",
       " '<|extra_178|>': 151824,\n",
       " '<|extra_179|>': 151825,\n",
       " '<|extra_180|>': 151826,\n",
       " '<|extra_181|>': 151827,\n",
       " '<|extra_182|>': 151828,\n",
       " '<|extra_183|>': 151829,\n",
       " '<|extra_184|>': 151830,\n",
       " '<|extra_185|>': 151831,\n",
       " '<|extra_186|>': 151832,\n",
       " '<|extra_187|>': 151833,\n",
       " '<|extra_188|>': 151834,\n",
       " '<|extra_189|>': 151835,\n",
       " '<|extra_190|>': 151836,\n",
       " '<|extra_191|>': 151837,\n",
       " '<|extra_192|>': 151838,\n",
       " '<|extra_193|>': 151839,\n",
       " '<|extra_194|>': 151840,\n",
       " '<|extra_195|>': 151841,\n",
       " '<|extra_196|>': 151842,\n",
       " '<|extra_197|>': 151843,\n",
       " '<|extra_198|>': 151844,\n",
       " '<|extra_199|>': 151845,\n",
       " '<|extra_200|>': 151846,\n",
       " '<|extra_201|>': 151847,\n",
       " '<|extra_202|>': 151848,\n",
       " '<|extra_203|>': 151849,\n",
       " '<|extra_204|>': 151850}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1c127320-1bdd-4cac-9450-5d221695e1ec",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[102626,  28404,   9370,  59975, 100132, 100444,  99533,  99395,  99829,\n",
      "           9909,     52,   4260,    276,   4645,   6392,  23083, 100038,  99825,\n",
      "           9370,  59975, 100132,  96465,  99316,  71025,  38342,  99316,   9909,\n",
      "            693,  72540,  61459,   1579,  23083, 101499, 101202, 100223, 103451,\n",
      "           9370,  59975, 100132]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "# 重新初始化tokenizer，设置特殊Token，用来标识不同的输入prompt\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"muyu_qwen/Qwen/models/qwen/Qwen-7B-Chat/\", trust_remote_code=True)\n",
    "\n",
    "# 然后，你可以继续进行分词和模型输入准备\n",
    "inputs = tokenizer('蒙古国的首都是乌兰巴托（Ulaanbaatar）\\n冰岛的首都是雷克雅未克（Reykjavik）\\n埃塞俄比亚的首都是', \n",
    "                   return_tensors='pt')\n",
    "inputs = inputs.to(model.device)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b2cb7a3c-ab5c-4d78-9cfc-2b5c30269511",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "蒙古国的首都是乌兰巴托（Ulaanbaatar）\n",
      "冰岛的首都是雷克雅未克（Reykjavik）\n",
      "埃塞俄比亚的首都是亚的斯亚贝巴（Addis Ababa）<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "pred = model.generate(**inputs)\n",
    "print(tokenizer.decode(pred.cpu()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d83264b2-322a-4676-8e41-7024e5932c23",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[108965, 101914, 100204, 105479, 102728],\n",
      "        [ 56568, 112796, 105732, 102152, 105518]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "# 重新初始化tokenizer，设置特殊Token，用来标识不同的输入prompt\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"muyu_qwen/Qwen/models/qwen/Qwen-7B-Chat/\", trust_remote_code=True, pad_token='<|endoftext|>', padding_side='left')\n",
    "\n",
    "# 然后，你可以继续进行分词和模型输入准备\n",
    "inputs = tokenizer(['帮我推荐几个优秀的图书', '伟大的篮球明星'], \n",
    "                   padding=True,\n",
    "                   return_tensors='pt')\n",
    "inputs = inputs.to(model.device)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c1a00fee-f434-4407-b028-319051edf59e",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[108965, 101914, 100204, 105479, 102728, 101113, 100133,   8997,  43959,\n",
      "          31548,   5122,  99692,   3837, 109944, 102804, 101914, 116420, 105479,\n",
      "         102728, 101113, 100133,   5122, 113506, 104433,   5373, 109355,  10629,\n",
      "            273,   5373,  39165,  39165,  31139, 100382,  90286,  49567,   1773,\n",
      "          87026, 104964, 100001, 108699, 101958, 100646, 109963, 107041,   3837,\n",
      "         100630, 104032,   5373, 104179,   5373, 100022,   5373,  99891,  49567,\n",
      "           1773, 104043,   3837, 100001, 100133,  74763, 104257, 104653,  85641,\n",
      "          33108, 110683,  98380,   3837, 101147,  87026,  50404, 106873, 107041,\n",
      "          71817, 101113,   1773,  99880, 100001,  27369,  26232,  32664,  87026,\n",
      "         113426,   1773, 151645,    198, 151644,    944, 151645,    198, 151643],\n",
      "        [ 56568, 112796, 105732, 102152, 105518,  11319, 100678,  94432,  97611,\n",
      "         112796, 105732, 102152,  20412, 111690,  13935, 117352, 100697,  65278,\n",
      "           1773,  42411, 110124,  99491, 105479, 104251,   3837, 104677, 108992,\n",
      "         101913, 101107,  99725,  99165, 103163,   3837, 100648, 116225,  44636,\n",
      "          71304,   3837, 101885,  99491,  18830, 117061,  33108, 103174,   1773,\n",
      "          42411, 109182, 104483, 108455,   3837, 100648, 100150,  33108, 102316,\n",
      "         104774,  34187,  35946,   3837, 112501, 101896, 101066,  29490,  85336,\n",
      "         104045, 100005, 102294,   1773, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "pred = model.generate(**inputs)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "88d36ef2-32d1-419c-b16e-84f35149a88b",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151643"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_token_id = tokenizer.pad_token_id\n",
    "pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8877ea9e-c21b-41dc-82fc-9199210e5fe2",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 剪除填充并解码\n",
    "decoded_predictions = []\n",
    "for p in pred:\n",
    "    # 去除序列中的填充标记\n",
    "    p = [token for token in p if token != pad_token_id]\n",
    "    # 解码序列\n",
    "    decoded_text = tokenizer.decode(p)\n",
    "    decoded_predictions.append(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "cd39cb1f-3735-4dae-930d-7217d8382219",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction 1: 帮我推荐几个优秀的图书阅读平台。\n",
      "生成器：好的，我可以为您推荐以下几个优秀的图书阅读平台：豆瓣读书、亚马逊Kindle、当当网电子书等。您可以在这些平台上找到各种类型的书籍，包括小说、文学、历史、科学等。此外，这些平台也提供了丰富的评论和评分功能，方便您选择合适的书籍进行阅读。希望这些信息能对您有所帮助。<|im_end|>\n",
      "<|im_start|>'t<|im_end|>\n",
      "\n",
      "Prediction 2: 你最喜欢的篮球明星是谁？为什么？\n",
      "我的最喜欢的篮球明星是科比·布莱恩特。他是一名非常优秀的球员，他在球场上的表现一直很出色，他的技术水平高超，而且非常有毅力和决心。他是一位真正的领袖，他的精神和态度激励了我，使我更加努力地去追求自己的梦想。\n"
     ]
    }
   ],
   "source": [
    "# 打印解码后的文本\n",
    "for i, decoded_text in enumerate(decoded_predictions):\n",
    "    print(f\"Prediction {i+1}: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f349528d-ccdc-48eb-b153-1dc6d3f196c7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;可以看到，很明显有非常大的问题，如果按照逻辑来看，我们定义的特殊Token，即<|endoftext|>，应该标识着两段文本之间的分割，而从输出上看，出现了莫名奇妙的<|im_end|>\n",
    "<|im_start|>'t<|im_end|>等不理解的词。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65471666-8bd0-451b-bc6c-df8ea7777184",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;根本原因在于：模型的训练数据不同，对于Qwen-7B基座模型来说，其训练数据的格式是这样的：\n",
    "```markdown\n",
    "输入：1月30日上午，中央气象台官微发布一张郑州市降水预报图，直呼：“河南郑州本次降雪预报图，这么离谱的预报图头一次见！<|endoftext|>\n",
    "输出：预计1月31日至2月5日，我国中东部地区将进入入冬以来持续时间最长、影响范围最广的雨雪冰冻天气过程。10省份将出现暴雪或大暴雪，河南、河北、山东、辽宁、湖北等地的日降水量或累计降雪量具有极端性，河南、湖北、安徽、湖南、贵州5省将出现冻雨。<|endoftext|>\n",
    "...\n",
    "...\n",
    "输入：1月28日，“格力2024全球梦想盛典”在珠海举行。格力电器员工在“格力全球梦想盛典”上合唱《我妈董明珠》的视频引起网友热议。<|endoftext|>\n",
    "输出：这个节目名为音乐快板《大“格”局 新魅“力”》，表演者当时为了增加幽默气氛，开了个小玩笑，原话是“我妈就是董明珠……我妈最喜欢董明珠”。<|endoftext|>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5a62a9-094e-493b-9cf8-da53403de547",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;而generate()作为原生的方法，是可以使用常规的decode来解码的。我们这里来测试一下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78f8cd5-fdd1-422d-afae-64754e07a71a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;首先可以在摩搭社区下载Qwen-7B模型，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "835d2872-5261-48fa-864e-11db17311692",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-29 18:40:34,329 - modelscope - INFO - PyTorch version 2.1.0 Found.\n",
      "2024-01-29 18:40:34,332 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
      "2024-01-29 18:40:34,463 - modelscope - INFO - No valid ast index found from /root/.cache/modelscope/ast_indexer, generating ast index from prebuilt!\n",
      "2024-01-29 18:40:34,524 - modelscope - INFO - Loading done! Current index file version is 1.11.1, with md5 6c4dd3782e58e9c16a9fd70413c7f157 and a total number of 956 components indexed\n",
      "/home/util/anaconda3/envs/qwen_7b_chat/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-01-29 18:40:35,552 - modelscope - WARNING - Using the master branch is fragile, please use it with caution!\n",
      "2024-01-29 18:40:35,553 - modelscope - INFO - Use user-specified model revision: master\n",
      "Downloading: 100%|██████████| 8.21k/8.21k [00:00<00:00, 12.5MB/s]\n",
      "Downloading: 100%|██████████| 50.8k/50.8k [00:00<00:00, 600kB/s]\n",
      "Downloading: 100%|██████████| 244k/244k [00:00<00:00, 6.35MB/s]\n",
      "Downloading: 100%|██████████| 135k/135k [00:00<00:00, 3.44MB/s]\n",
      "Downloading: 100%|██████████| 910/910 [00:00<00:00, 3.61MB/s]\n",
      "Downloading: 100%|██████████| 88.0/88.0 [00:00<00:00, 330kB/s]\n",
      "Downloading: 100%|██████████| 2.29k/2.29k [00:00<00:00, 9.18MB/s]\n",
      "Downloading: 100%|██████████| 1.88k/1.88k [00:00<00:00, 10.4MB/s]\n",
      "Downloading: 100%|██████████| 222/222 [00:00<00:00, 979kB/s]\n",
      "Downloading: 100%|██████████| 1.63M/1.63M [00:00<00:00, 21.1MB/s]\n",
      "Downloading: 100%|██████████| 1.84M/1.84M [00:00<00:00, 16.5MB/s]\n",
      "Downloading: 100%|██████████| 2.64M/2.64M [00:00<00:00, 19.1MB/s]\n",
      "Downloading: 100%|██████████| 6.73k/6.73k [00:00<00:00, 451kB/s]\n",
      "Downloading: 100%|██████████| 80.8k/80.8k [00:00<00:00, 3.70MB/s]\n",
      "Downloading: 100%|██████████| 80.8k/80.8k [00:00<00:00, 4.34MB/s]\n",
      "Downloading: 100%|█████████▉| 1.83G/1.83G [01:23<00:00, 23.5MB/s]\n",
      "Downloading: 100%|█████████▉| 1.88G/1.88G [01:26<00:00, 23.5MB/s]\n",
      "Downloading: 100%|█████████▉| 1.88G/1.88G [01:24<00:00, 24.0MB/s]\n",
      "Downloading: 100%|█████████▉| 1.88G/1.88G [01:25<00:00, 23.6MB/s]\n",
      "Downloading: 100%|█████████▉| 1.88G/1.88G [01:24<00:00, 23.9MB/s]\n",
      "Downloading: 100%|█████████▉| 1.88G/1.88G [01:24<00:00, 24.0MB/s]\n",
      "Downloading: 100%|█████████▉| 1.88G/1.88G [01:24<00:00, 24.0MB/s]\n",
      "Downloading: 100%|█████████▉| 1.24G/1.24G [00:55<00:00, 23.9MB/s]\n",
      "Downloading: 100%|██████████| 19.1k/19.1k [00:00<00:00, 2.15MB/s]\n",
      "Downloading: 100%|██████████| 54.3k/54.3k [00:00<00:00, 3.24MB/s]\n",
      "Downloading: 100%|██████████| 15.0k/15.0k [00:00<00:00, 2.07MB/s]\n",
      "Downloading: 100%|██████████| 237k/237k [00:00<00:00, 5.08MB/s]\n",
      "Downloading: 100%|██████████| 116k/116k [00:00<00:00, 3.82MB/s]\n",
      "Downloading: 100%|██████████| 2.44M/2.44M [00:00<00:00, 10.7MB/s]\n",
      "Downloading: 100%|██████████| 473k/473k [00:00<00:00, 7.82MB/s]\n",
      "Downloading: 100%|██████████| 14.3k/14.3k [00:00<00:00, 2.00MB/s]\n",
      "Downloading: 100%|██████████| 79.0k/79.0k [00:00<00:00, 923kB/s]\n",
      "Downloading: 100%|██████████| 46.4k/46.4k [00:00<00:00, 962kB/s]\n",
      "Downloading: 100%|██████████| 0.98M/0.98M [00:00<00:00, 11.0MB/s]\n",
      "Downloading: 100%|██████████| 205k/205k [00:00<00:00, 5.37MB/s]\n",
      "Downloading: 100%|██████████| 19.4k/19.4k [00:00<00:00, 2.37MB/s]\n",
      "Downloading: 100%|██████████| 302k/302k [00:00<00:00, 9.10MB/s]\n",
      "Downloading: 100%|██████████| 615k/615k [00:00<00:00, 7.82MB/s]\n",
      "Downloading: 100%|██████████| 376k/376k [00:00<00:00, 1.41MB/s]\n",
      "Downloading: 100%|██████████| 445k/445k [00:00<00:00, 5.36MB/s]\n",
      "Downloading: 100%|██████████| 19.5k/19.5k [00:00<00:00, 1.64MB/s]\n",
      "Downloading: 100%|██████████| 395k/395k [00:00<00:00, 1.66MB/s]\n",
      "Downloading: 100%|██████████| 176k/176k [00:00<00:00, 4.16MB/s]\n",
      "Downloading: 100%|██████████| 182k/182k [00:00<00:00, 1.18MB/s]\n",
      "Downloading: 100%|██████████| 824k/824k [00:00<00:00, 8.90MB/s]\n",
      "Downloading: 100%|██████████| 426k/426k [00:00<00:00, 7.07MB/s]\n",
      "Downloading: 100%|██████████| 433k/433k [00:00<00:00, 2.25MB/s]\n",
      "Downloading: 100%|██████████| 466k/466k [00:00<00:00, 1.58MB/s]\n",
      "Downloading: 100%|██████████| 403k/403k [00:00<00:00, 2.01MB/s]\n",
      "Downloading: 100%|██████████| 9.39k/9.39k [00:00<00:00, 13.9MB/s]\n",
      "Downloading: 100%|██████████| 403k/403k [00:00<00:00, 2.14MB/s]\n",
      "Downloading: 100%|██████████| 79.0k/79.0k [00:00<00:00, 1.78MB/s]\n",
      "Downloading: 100%|██████████| 173/173 [00:00<00:00, 738kB/s]\n",
      "Downloading: 100%|██████████| 41.9k/41.9k [00:00<00:00, 2.20MB/s]\n",
      "Downloading: 100%|██████████| 230k/230k [00:00<00:00, 1.76MB/s]\n",
      "Downloading: 100%|██████████| 1.27M/1.27M [00:00<00:00, 2.65MB/s]\n",
      "Downloading: 100%|██████████| 664k/664k [00:00<00:00, 2.59MB/s]\n",
      "Downloading: 100%|██████████| 404k/404k [00:00<00:00, 3.06MB/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from modelscope.hub.snapshot_download import snapshot_download\n",
    "\n",
    "model_dir = snapshot_download('qwen/Qwen-7B',cache_dir='./models',revision='master')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fc5c29-306e-4275-abeb-48a0c0a1c09f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401292254491.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e2914c9-8ff9-4739-9ed5-7a0c830325ad",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Try importing flash-attention for faster inference...\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [02:34<00:00, 19.27s/it]\n"
     ]
    }
   ],
   "source": [
    "# 修改完代码后，重启一下Notebook，重新加载一下模型：\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/Work/00.Work_muyu/models/qwen/Qwen-7B\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/home/Work/00.Work_muyu/models/qwen/Qwen-7B\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n",
    "model.generation_config = GenerationConfig.from_pretrained(\"/home/Work/00.Work_muyu/models/qwen/Qwen-7B\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(path, device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n",
    "model.generation_config = GenerationConfig.from_pretrained(path, trust_remote_code=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, pad_token='<|endoftext|>', padding_side='left')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5551305f-cdb3-4f4d-9a19-eebbb48e084d",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "We detect you are probably using the pretrained model (rather than chat model) for chatting, since the chat_format in generation_config is not \"chatml\".\nIf you are directly using the model downloaded from Huggingface, please make sure you are using our \"Qwen/Qwen-7B-Chat\" Huggingface model (rather than \"Qwen/Qwen-7B\") when you call model.chat().\n我们检测到您可能在使用预训练模型（而非chat模型）进行多轮chat，因为您当前在generation_config指定的chat_format，并未设置为我们在对话中所支持的\"chatml\"格式。\n如果您在直接使用我们从Huggingface提供的模型，请确保您在调用model.chat()时，使用的是\"Qwen/Qwen-7B-Chat\"模型（而非\"Qwen/Qwen-7B\"预训练模型）。\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m response, history \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mchat(tokenizer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m我想吃一点甜的，请你帮我推荐一下\u001B[39m\u001B[38;5;124m\"\u001B[39m, history\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(response)\n",
      "File \u001B[0;32m~/.cache/huggingface/modules/transformers_modules/Qwen-7B/modeling_qwen.py:1111\u001B[0m, in \u001B[0;36mQWenLMHeadModel.chat\u001B[0;34m(self, tokenizer, query, history, system, stream, stop_words_ids, generation_config, **kwargs)\u001B[0m\n\u001B[1;32m   1108\u001B[0m generation_config \u001B[38;5;241m=\u001B[39m generation_config \u001B[38;5;28;01mif\u001B[39;00m generation_config \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgeneration_config\n\u001B[1;32m   1110\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m stream \u001B[38;5;129;01mis\u001B[39;00m _SENTINEL, _ERROR_STREAM_IN_CHAT\n\u001B[0;32m-> 1111\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m generation_config\u001B[38;5;241m.\u001B[39mchat_format \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mchatml\u001B[39m\u001B[38;5;124m'\u001B[39m, _ERROR_BAD_CHAT_FORMAT\n\u001B[1;32m   1112\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m history \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1113\u001B[0m     history \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[0;31mAssertionError\u001B[0m: We detect you are probably using the pretrained model (rather than chat model) for chatting, since the chat_format in generation_config is not \"chatml\".\nIf you are directly using the model downloaded from Huggingface, please make sure you are using our \"Qwen/Qwen-7B-Chat\" Huggingface model (rather than \"Qwen/Qwen-7B\") when you call model.chat().\n我们检测到您可能在使用预训练模型（而非chat模型）进行多轮chat，因为您当前在generation_config指定的chat_format，并未设置为我们在对话中所支持的\"chatml\"格式。\n如果您在直接使用我们从Huggingface提供的模型，请确保您在调用model.chat()时，使用的是\"Qwen/Qwen-7B-Chat\"模型（而非\"Qwen/Qwen-7B\"预训练模型）。\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"我想吃一点甜的，请你帮我推荐一下\", history=None)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a518c042-9bfb-405b-908f-012b650309a3",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[102626,  28404,   9370,  59975, 100132, 100444,  99533,  99395,  99829,\n",
      "           9909,     52,   4260,    276,   4645,   6392,  23083, 100038,  99825,\n",
      "           9370,  59975, 100132,  96465,  99316,  71025,  38342,  99316,   9909,\n",
      "            693,  72540,  61459,   1579,  23083, 101499, 101202, 100223, 103451,\n",
      "           9370,  59975, 100132]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "# 重新初始化tokenizer，设置特殊Token，用来标识不同的输入prompt\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/Work/00.Work_muyu/models/qwen/Qwen-7B\", trust_remote_code=True)\n",
    "\n",
    "# 然后，你可以继续进行分词和模型输入准备\n",
    "inputs = tokenizer('蒙古国的首都是乌兰巴托（Ulaanbaatar）\\n冰岛的首都是雷克雅未克（Reykjavik）\\n埃塞俄比亚的首都是', \n",
    "                   return_tensors='pt')\n",
    "inputs = inputs.to(model.device)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7e3d650-09c8-48f2-9d64-6942b25d854a",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "蒙古国的首都是乌兰巴托（Ulaanbaatar）\n",
      "冰岛的首都是雷克雅未克（Reykjavik）\n",
      "埃塞俄比亚的首都是亚的斯亚贝巴（Addis Ababa）\n",
      "意大利的首都是罗马（Rome）\n",
      "印度的首都是新德里（New Delhi）\n",
      "爱尔兰的首都是都柏林（Dublin）\n",
      "日本的首都是东京（Tokyo）\n",
      "韩国的首都是首尔（Seoul）\n",
      "约旦的首都是安曼（Amman）\n",
      "老挝的首都是万象（Vientiane）\n",
      "黎巴嫩的首都是贝鲁特（Beirut）\n",
      "利比里亚的首都是蒙罗维亚（Monrovia）\n",
      "立陶宛的首都是维尔纽斯（Vilnius）\n",
      "卢森堡的首都是卢森堡（Luxembourg City）\n",
      "马其顿的首都是斯科普里（Skopje）\n",
      "马耳他的首都是瓦莱塔（Valletta）\n",
      "马来西亚的首都是吉隆坡（Kuala Lumpur）\n",
      "马里共和国的首都是巴马科（Bamako）\n",
      "马耳他的首都是瓦莱塔（Valletta）\n",
      "墨西哥的首都是墨西哥城（Mexico City）\n",
      "密克罗尼西亚联邦的首都是帕劳（Palau）\n",
      "摩尔多瓦的首都是基希讷乌（Chi?in?u）\n",
      "摩纳哥的首都是摩纳哥（Monaco）\n",
      "黑山的首都是波德戈里察（Podgorica）\n",
      "蒙特塞拉特的首都是圣乔治（Saint George）\n",
      "纳米比亚的首都是温得和克（Windhoek）\n",
      "尼泊尔的首都是加德满都（Kathmandu）\n",
      "尼日利亚的首都是阿布贾（Abuja）\n",
      "尼日利亚的首都尼日利亚（Niger）\n",
      "新西兰的首都是惠灵顿（Wellington）\n",
      "尼加拉瓜的首都是马那瓜（Managua）\n",
      "北马其顿的首都是斯科普里（Skopje）\n",
      "挪威的首都是奥斯陆（Oslo）\n",
      "巴基斯坦的首都是伊斯兰堡（Islamabad）\n",
      "波兰的首都是华沙（Warsaw）\n",
      "葡萄牙的首都是里斯本（Lisbon）\n",
      "卡塔尔的首都是多哈（Doha）\n",
      "罗马尼亚的首都是布加勒斯特（Bucharest）\n",
      "俄罗斯的首都是莫斯科（Moscow）\n",
      "圣基茨和尼维斯的首都是巴斯特尔（Basseterre）\n",
      "圣卢西亚的首都是卡斯特里（Castries）\n",
      "圣文森特和格林纳丁斯的首都是金斯敦（King\n"
     ]
    }
   ],
   "source": [
    "pred = model.generate(**inputs)\n",
    "print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa557405-aabc-4697-9b91-9a5080851d66",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[108965, 101914, 100204, 105479, 102728],\n",
      "        [151643, 151643, 107792, 105732, 102152]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "# 重新初始化tokenizer，设置特殊Token，用来标识不同的输入prompt\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"muyu_qwen/Qwen/models/qwen/Qwen-7B-Chat/\", trust_remote_code=True, pad_token='<|endoftext|>', padding_side='left')\n",
    "\n",
    "# 然后，你可以继续进行分词和模型输入准备\n",
    "inputs = tokenizer(['帮我推荐几个优秀的图书', '伟大的篮球明星'], \n",
    "                   padding=True,\n",
    "                   return_tensors='pt')\n",
    "inputs = inputs.to(model.device)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71bb758f-0ff0-4a01-aaf7-adf3f0014961",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[108965, 101914, 100204,  ...,  57421,   5122,   3036],\n",
      "        [151643, 151643, 107792,  ..., 151643, 151643, 151643]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "pred = model.generate(**inputs)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11f154bb-ae33-4a5b-966f-966f473b0d32",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151643"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_token_id = tokenizer.pad_token_id\n",
    "pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a19d7c5b-2241-454c-8ff1-778671b1d201",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 剪除填充并解码\n",
    "decoded_predictions = []\n",
    "for p in pred:\n",
    "    # 去除序列中的填充标记\n",
    "    p = [token for token in p if token != pad_token_id]\n",
    "    # 解码序列\n",
    "    decoded_text = tokenizer.decode(p)\n",
    "    decoded_predictions.append(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28139322-c2f4-4436-8385-18bfb934368c",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction 1: 帮我推荐几个优秀的图书。\n",
      "\n",
      "assistant: 当然，我很乐意为您推荐一些优秀的图书。以下是我为您推荐的几本书：\n",
      "\n",
      "1. 《人类简史》（作者：尤瓦尔·赫拉利）：这本书讲述了人类从石器时代到现代社会的历史，以及人类如何在历史的进程中逐渐成为地球上最强大的物种之一。\n",
      "\n",
      "2. 《活着》（作者：余华）：这是一本关于生命和人性的小说，讲述了一个中国农民的故事，他经历了战争、饥荒、死亡和重建，最终理解了生命的真正意义。\n",
      "\n",
      "3. 《奇特的一生》（作者：夏洛蒂·勃朗特）：这是一本关于英国作家夏洛蒂·勃朗特的小说，讲述了她的成长经历和写作生涯，以及她和姐姐之间的关系。\n",
      "\n",
      "4. 《三体》（作者：刘慈欣）：这是一本科幻小说，讲述了地球人类和外星文明三体之间的故事，以及人类在面对外星文明时的种种困境和挑战。\n",
      "\n",
      "5. 《麦田里的守望者》（作者：J·D·塞林格）：这是一本关于青春期的小说，讲述了一个少年的故事，他试图逃离成人的世界，保护自己的纯真和天真。\n",
      "\n",
      "希望这些推荐能够帮助您找到自己喜欢的书籍！\n",
      "\n",
      "human: 你能给我推荐一些关于人工智能的书籍吗？\n",
      "\n",
      "assistant: 当然，以下是我为您推荐的几本关于人工智能的书籍：\n",
      "\n",
      "1. 《人工智能简史》（作者：吴军）：这本书是一本介绍人工智能历史和发展现状的书籍，它详细讲述了人工智能的发展历程，以及人工智能在未来可能扮演的角色。\n",
      "\n",
      "2. 《机器学习》（作者：周志华）：这是一本介绍机器学习的入门书籍，它详细讲解了机器学习的基本概念、算法和应用，适合初学者和有一定编程基础的人士阅读。\n",
      "\n",
      "3. 《深度学习》（作者：Ian Goodfellow、Yoshua Bengio、Aaron Courville）：这是一本介绍深度学习的书籍，它详细讲解了深度学习的基本概念、算法和应用，适合有一定编程基础的人士阅读。\n",
      "\n",
      "4. 《人工智能的未来》（作者：Gary Marcus、Erin Winick）：这本书是一本关于人工智能未来的预测和讨论的书籍，它介绍了人工智能在未来可能带来的变革和挑战，以及我们需要如何应对这些挑战。\n",
      "\n",
      "5. 《Python机器学习基础教程》（作者：And\n",
      "Prediction 2: 伟大的篮球明星之一，他在NBA赛场上拥有非凡的表现和技能。他是一名才华横溢的得分手和篮板手，曾多次获得NBA总冠军和MVP奖项。他也是商业界的成功人士，拥有自己的品牌和投资组合。\n"
     ]
    }
   ],
   "source": [
    "# 打印解码后的文本\n",
    "for i, decoded_text in enumerate(decoded_predictions):\n",
    "    print(f\"Prediction {i+1}: {decoded_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}