{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d43f845b-1999-4691-bb16-9dea1489a717",
   "metadata": {},
   "source": [
    "# **深度学习公开课 - NLP与GPT&GLM大模型技术**\n",
    "> 节选自《2023深度学习臻选夏季班》正课<br>\n",
    "> 作者：@菜菜TsaiTsai<br>\n",
    "> 版本号：2023/6/13<br>"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "1"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "88484b137fe5414a"
  },
  {
   "cell_type": "markdown",
   "id": "bb938a86-08f7-4c1c-ae41-f86277167fa8",
   "metadata": {},
   "source": [
    "## **<center><font color =\"k\">公开课Day1 循环神经网络与NLP发展前景<br><br>极致易学 | 高效入门 | 前景讨论<center>**<br>**<center><font color =\"red\">8点10分开始～<br><br>扫码回复\"DL999\"领取今日直播竞赛数据/课件>>></font></center>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa32fb3-dfbd-41d4-97d8-4700a08d2e64",
   "metadata": {},
   "source": [
    "- 节选自《2023深度学习臻选夏季班》正式课程\n",
    "\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566226fb-9977-4dd5-9c1f-0cdf714a16d8",
   "metadata": {},
   "source": [
    "## 1 欢迎来到NLP的世界！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2facdcab-c2eb-4486-bc68-d50b6bdcb232",
   "metadata": {},
   "source": [
    "无论你是经验丰富的深度算法工程师，还是仅仅简单了解过人工智能这一概念的算法入门者，你一定都听说过大名鼎鼎的“图灵测试”。图灵测试是计算机科学家阿兰·图灵在1950年提出的概念，这一测试可以衡量机器是否能够表现出与人类相当的智能。具体来说，图灵测试让一个人类与另一个不可视房间中存在的机器**进行对话**，如果该人类无法判断出与他/她对话的对象是机器还是真人，那该机器就被认为“通过图灵测试”，即被认可“拥有人工智能”。\n",
    "\n",
    "![](https://media.licdn.com/dms/image/D4D12AQHjcIMCb7sTfQ/article-cover_image-shrink_600_2000/0/1679513310072?e=2147483647&v=beta&t=NsZ2UMVjgOgZRwZvG3tItLS5Xs9-pMbZYIbUeLYDWYo)\n",
    "\n",
    "自图灵测试被提出70多年以来，它一直是深度学习研究者们津津乐道的主题之一，众多模型都被投入进行不严格的“图灵测试”，而人们也以“模型是否与真人足够相似”、“模型生成的文字能否与假乱真”来衡量模型的优异程度。不公平的是，当人工智能在非语言领域表现卓越、甚至远远超出人类平均水平时，人们往往习以为常——例如，广泛应用的人脸识别技术其实并不出圈，大部分普通人心中所想的“人工智能”与人脸识别机器大相径庭；AI绘画虽然技术成熟、但之前只在小范围内爆发、并未掀起社会性的共鸣。然而，当人工智能的交流能力接近人类水平、甚至还不能超越人类水平时（例如：ChatGPT的爆火），社会人就已经感觉到了巨大的威胁，国家紧急出台人工智能应用标准、学者与企业家发起“暂停研发”的呼吁、全球开启AI军备竞赛等技术大事件接踵而至，一时之间热闹非凡。\n",
    "\n",
    "无论是图灵测试的设计方式，还是GPT爆火引发的AI浪潮都说明——在人工智能发展的过程当中，深度学习学者们、甚至整个人类社会都无意识地达成了一种高度的共识：**交流能力（Communication）是智能的终极体现，人机同频的交流是智能被实现的象征**，无论一个人工智能算法有多强大的能力。只要它不能普适性地理解人类、不能让人类理解、不能与人类顺畅交流，它终归是无法融入人类和商业社会的（残酷的是，一个真人也是一样）。人工智能的终极评判标准，就是人机同频交流。\n",
    "\n",
    "在“人机同频交流”的大目标下，自然语言处理（Natural Language Process）这一领域的关键性不言而喻。**人类90%的信息获取与交流都依赖于语言，人类所有的逻辑、情感、知识、智慧、甚至社会的构建、文明的传承依赖于对语言的理解和表达**。因此，计算机想要具备“看人类所看，想人类所想，与人类同频”的能力，就必须理解人类所使用的自然语言，而自然语言处理（Natural Langurage Process）正是研究**如何让计算机认知人类语言、理解人类语言、生成人类语言、甚至依赖这些语言与人进行交流、完成特定语言任务**的关键学科。豪不夸张的说，人工智能能否真正“智能”，很大程度上都依赖于自然语言处理领域的发展。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d193b1b1-d52e-44b2-8daa-bda839e76115",
   "metadata": {},
   "source": [
    "- **NLP的三大发展阶段**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa84525a-c73c-4902-89d3-a6de33aea4b4",
   "metadata": {},
   "source": [
    "从2011年第三次人工智能革命开启，自然语言处理领域已经经历了三大发展阶段：\n",
    "\n",
    "1. 探索阶段：2011～2015（前Transformer时代）\n",
    "\n",
    "在AlphaGo和卷积网络掀起第三次人工智能革命之前，NLP领域主要依赖人工规则和知识库构建非常精细的“规则类语言模型”，当人工智能浪潮来临后，NLP转向使用统计学模型、深度学习模型和大规模语料库。**在这个阶段，NLP领域的重要目标是“研发语言模型、找出能够处理语言数据的算法”**。因此在这个阶段，NLP领域学者们一直在尝试一些重要的技术和算法，如隐马尔可夫模型（HMM）、条件随机场（CRF）和支持向量机（SVM）。同时，这个阶段也见证了循环神经网络RNN和长短期记忆网络LSTM等神经网络模型的出现和发展。\n",
    "\n",
    "2. 提升阶段：2015～2020（Transformer时代）\n",
    "\n",
    "RNN和LSTM是非常有效的语言模型，但是和在视觉领域大放光彩的卷积网络比起来，RNN对语言的处理能力只能达到“小规模数据上勉强够用”的程度。2015年谷歌将自注意力机制发扬光大、提出了Transformer架构，在未来的几年中，基于transformer的BERT、GPT等语言模型相继诞生，因此**这个阶段NLP领域的重要目标是“大幅提升语言模型在自然语言理解和生成方面的能力”**。这是自然语言处理理论发展最辉煌的时代之一。此外，这个阶段中语言模型已经能够很好地完成NLP领域方面的各个任务，因此工业界也实现了不少语言模型的应用，比如搜索引擎、推荐系统、自动翻译、智能助手等。\n",
    "\n",
    "3. 应用阶段：2020-至今（大模型时代）\n",
    "\n",
    "2020年秋天、GPT3.0所写的小软文在社交媒体上爆火，这个总参数量超出1750w、每运行1s就要消耗100w美元的大语言模型（Large Language Models，LLMs）为NLP领域开启了一个全新的阶段。在这一阶段，大模型在许多NLP任务上取得了前所未有好成绩，在模型精度、模型泛化能力、复杂任务处理能力方面都展示出了难以超越的高水准，这吸引了大量资本的注意、同时也催生了NLP领域全新的发展方向与研究方向。相比起2020年前百花齐放、理论极速发展的研究阶段，现阶段NLP领域的核心目标主要集中在大模型研发 & 大模型技术变现两大方向上：\n",
    "\n",
    "> 1) 如何研发、训练自己的大模型？\n",
    "\n",
    "虽然GPT系列大模型的原理并未开源，但GPT的成功无疑为“如何提升语言模型表现”指出了一条明路。在GPT的启发下，海内外各大科技企业正在研发基于BERT、基于GPT或基于Transformer其他组合方式的大模型，国内一线大模型ChatGLM系列就是基于BERT和GPT的融合理念开发的中文大模型。同时，大模型研发和训练技术、如生物反馈式强化学习（RLFH）、近端策略优化（PPO）、奖励权重策略（Reward-based Weighting）、DeepSpeed训练引擎等发展迅速，势不可挡。虽然现在已不是NLP理论发展的高峰，但毫无疑问，大模型算法研发与训练依然是NLP最前沿的研究方向之一。\n",
    "\n",
    "> 2) 如何降低大模型应用门槛与应用成本？\n",
    "\n",
    "大模型吞吃大量语料、训练成本极高，要将大模型应用到具体商业场景、还需进一步研究和训练。因此降低大模型应用成本的预训练、微调、大规模语料库构建等技术正蓬勃发展！自2020年以来已诞生十余种可行的微调方法和自动语料生成方法，如有监督微调（SFT）、低阶自适应微调方法LoRA、提示词前缀微调方法Prefix Tuning、轻量级Prefix微调Prompt Tuning、百倍效率提升的微调方法P-Tuning V2、以及自适应预算分配微调方法AdaLoRA等。这些方法催生了GPT4.0和大量语言方面落地应用，已经大大改变了NLP的研究和应用格局。\n",
    "\n",
    "> 3) 如何化技术为产品，实现大语言模型的商业应用？\n",
    "\n",
    "大语言模型在变现方面有两大优势：首先，大语言模型的性能十分强大、足以很好地支持各类NLP方面服务；其次，大语言模型使用自然语言与消费者交互，可以大幅降低新产品的使用门槛，还可以与图像、语音等领域强势联动、形成多模态的产品。基于这两点变现优势，自动翻译、智能助手、文本分析、情感分析等经典NLP任务都有了实用且价格低廉的APP产品，人们在日常生活工作中更是有无限的机会接触到各类基于大模型技术的NLP应用，家庭物联网、语音指令等技术更是已经走入千家万户，一些谐星的领域，如AI算命、AI佛祖、AI心理咨询师等也相继诞生……\n",
    "\n",
    "同时，随着大模型应用门槛和使用门槛都逐步降低，大量的大模型产品不断涌现——ChatGPT、跨语言代码编译工具Cursor、Github官方代码编写工具CopilotX、一键生成PPT内容的Gamma AI，office全家桶中配置的Copilot、Photoshop中配置的fill features，广泛生成图像的MidJourney和Stable Diffusion……这些应用不仅改变了商业的运营方式，也极大地影响了人们的生活和工作。同时，大模型APP研发范式LangChain也受到了大规模追捧，LangChain正在逐步构建基于大模型研发变现产品的行业规范，很快整个人工智能领域都将迎来大规模变现的时代。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4763440-3d5e-49a4-9f60-64276060b3ba",
   "metadata": {},
   "source": [
    "- **NLP领域的机遇与危险**\n",
    "\n",
    "> 整体行业欣欣向荣，但行业结构会发生变化<br><br>\n",
    "> 核心技术发生转移、论文发表难度会上升<br><br>\n",
    "> 不会爆发针对NLPers的大规模失业潮（毕竟大部分NLPers在理论大爆发的年代做的也不是理论研究工作）<br><br>\n",
    "> NLPers面临转型、掌握大模型技术的NLPers会成为抢手人才<br><br>\n",
    "> 怀抱决心入行，现在是好时机，不过技术重点会发生改变<br><br>\n",
    "> 根据选择的方向，现在可以略微轻算法、重应用，但算法原理依然要学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de53c27a-3355-4c66-8f05-409a5b1009a2",
   "metadata": {},
   "source": [
    "- **深度学习夏季班**课程安排"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b94444-f6d2-41e2-9577-1dc1f09d02c3",
   "metadata": {},
   "source": [
    "## 2 自然语言领域中的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad37ec0-d610-4b46-b70c-90030697ce59",
   "metadata": {},
   "source": [
    "在深度学习的世界中，某一领域的架构/算法往往是根据该领域中特定的数据状态设计出来的。例如，为了处理带有空间信息的图像数据，算法工程师们使用了能够处理空间信息的卷积操作来创造卷积神经网络；又比如，为了将充满噪音的数据转变成干净的数据，算法工程师们创造了能够吞吃噪音、输出纯净数据的自动编码器结构。因此，在了解每个领域的算法架构之前，我们最好先学习当前领域的数据特点和数据结构，在自然语言处理领域也是如此。\n",
    "\n",
    "自然语言领域的核心数据是**序列数据**，这是一种在样本与样本之间存在特定顺序、且这种特定顺序不能被轻易修改的数据。这是什么意思呢？在机器学习和普通深度神经网络的领域中我们所使用的数据是二维表。如下所示，在普通的二维表中，样本与样本之间是相互独立的，一个样本及其特征对应了唯一的标签，因此无论我们先训练1号样本、还是先训练7号样本、还是只训练数据集中的一部分样本，都不会从本质上改变数据的含义、许多时候也不会改变算法对数据的理解和学习结果。\n",
    "\n",
    "![01](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/01.png)\n",
    "\n",
    "但序列数据则不然，对序列数据来说，一旦**调换样本顺序**或**样本发生缺失**，数据的含义就会发生巨大变化。最典型的序列数据有以下几种类型：\n",
    "\n",
    "1. **文本数据（Text Data）**：文本数据中的样本的“特定顺序”是语义的顺序，也就是词与词、句子与句子、段落与段落之间的顺序。在语义环境中，词语顺序的变化或词语的缺失可能会彻底改变语义，例如——\n",
    "> **改变顺序**：事半功倍和事倍功半；曾国藩战太平天国时非常著名的典故：他将“屡战屡败”修改为“屡败屡战”，前者给人绝望，后者给人希望。<br><br>\n",
    "> **样本缺失**（对文本来说特指上下文缺失）：小猫睡在毛毯上，因为它很____。当我们在横线上填上不同的词时，句子的含义会发生变化。\n",
    "\n",
    "2. **时间序列数据（Time Series Data）**：时间序列数据中的“特定顺序”就是时间顺序，时序数据中的每个样本就是每个时间点，在不同时间点上存在着不同的标签取值，且这些标签取值常常用于描述某个变量随时间变化的趋势，因此样本之间的顺序不能随意改变。例如，股票价格、气温记录和心电图等数据，一旦改变样本顺序，就会破坏当前趋势，影响对未来时间下的标签的预测。\n",
    "\n",
    "3. **音频数据（Audio Data）**：音频数据大部分时候是文本数据的声音信号，此时音频数据中的“特定顺序”也是语义的顺序；当然，音频数据中的顺序也可能是音符的顺序，试想你将一首歌的旋律全部打乱再重新播放，那整首歌的旋律和听感就会完全丧失。\n",
    "\n",
    "4. **视频数据（Video Data）**：你知道动画是由一张张原画构成的吗？视频数据本质就是由一帧帧图像构成的，因此视频数据是图像按照特定顺序排列后构成的数据。和音频数据类似，如果将动画或电影中的画面顺序打乱再重新播放，那没有任何人能够理解视频的内容。\n",
    "\n",
    "类似的数据还有很多，例如DNA序列数据，从医学角度来说DNA测序的顺序不能被打乱，否则就会违背医学常识。除此之外，符号序列数据也是常见的序列数据，密码学、自动编码学、甚至自动编程的算法都对数据本身的逻辑有严格的要求。很明显，**在处理序列数据时，我们不仅要让算法理解每一个样本，还需要让算法学习到样本与样本之间的联系**。今天，这些能够学习到样本之间联系的算法们构成了自然语言处理架构群。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e06c60-0a2a-420d-8841-57354d460e8e",
   "metadata": {},
   "source": [
    "- 序列数据的输入数据结构\n",
    "\n",
    "序列数据的概念很容易理解，但奇妙的是，现实中的序列数据可以是二、三、四、五任意维度，只要给原始的数据加上“时间顺序”或“位置顺序”，任意数据都可以化身为序列数据。在这里，我们展现几种常见的序列数据：\n",
    "\n",
    "> 二维时间序列\n",
    "\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/02.png)\n",
    "\n",
    "时间序列中，样本与样本之间的顺序是时间顺序，因此每个样本是一个时间点，时间顺序也就是time_step这一维度上的顺序。这种顺序在自然语言处理领域叫做“时间步”（time_step），也被叫做“序列长度”（sequence_length），这正是我们要求算法必须去学习的顺序。在时序数据中，时间点可以是任意时间单位（分钟、小时、天），但时间点与时间点之间的间隔必须是一致的。\n",
    "\n",
    "在NLP领域中，我们常常一次性处理多个时间序列，如下图所示，我们可以一次性处理多支股票的股价波动序列——"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e021c0-6a30-4ecd-8ab9-fc9816819c02",
   "metadata": {},
   "source": [
    "> 三维时间序列\n",
    "\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/03_.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0272e8-14d2-4357-9332-854fab733139",
   "metadata": {},
   "source": [
    "此时我们拥有的是一个三维矩阵，其中batch_size是样本量，也就是一共有多少个二维时间序列表单。你或许已经发现了，其实三维时间序列数据就是机器学习中定义的“多变量时间序列数据”。在多变量时间序列数据当中，时间和另一个因素共同决定唯一的特征值。在上面的例子中，每张时序二维表代表一支股票，因此在这个多变量时间序列数据中“时间”和“股票编号”共同决定了一个时间点上的值。相似的例子还可能是——不同用户在不同时间点上的行为，不同植物在不同季节时分泌的激素值、不同商家在不同时间点上的销售额等等……"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e045d50-2193-41e4-9a1a-16566ba1c881",
   "metadata": {},
   "source": [
    "> 二维文字序列\n",
    "\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/04.png)\n",
    "\n",
    "在文字数据中，样本与样本之间的联系大部分时候是词与词、字与字之间的联系，因此在文字序列中每个样本是一个单词或一个字（对英文来说大部分时候是一个单词，偶尔也可以是一个字母），故而**在中文文字数据中，一张二维表往往是一个句子或一段话**。此时，不能够打乱顺序的维度是vocab_size，它代表了一个句子\n",
    "/一段话中的字词总数量。一个句子或一段话越长，vocab_size也就会越大，因此这一维度的作用与时间序列中的time_step一致，vocab_size在许多时候也被称之为是序列长度（sequence_length）。同样，vocab_size这一维度上的顺序就是算法需要学习的顺序。\n",
    "\n",
    "需要注意的是，文字序列是不能直接放入算法进行运行的，必须要要编码成数字数据才能供算法学习，因此在NLP领域中我们大概率会将文字数据进行编码。编码的方式有很多种，但无一例外的，**文字编码的本质是用单一数字或一串数字的组合去代表某个字/词**，在同一套规则下，同一个字会被编码为同样的序列或同样的数字，而使用一个数字还是一串数字则可以由算法工程师自行决定。下图是对句子分别进行embedding编码和独热编码后产生的二维表单：\n",
    "\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce0afc5-14fa-491d-8182-3c3d3ad02c40",
   "metadata": {},
   "source": [
    "大部分时候，我们需要学习的肯定不止一个句子，当每个句子被编码成矩阵后，就会构成高维的多特征词向量。由于在实际训练时，所有句子或段落长度都一致的可能性太小（即所有句子的vocab_size都一致的可能性太小），因此我们往往为短句子进行填充、或将长句子进行裁剪，让所有的特征词向量保持在同样的维度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c94a64d-eb1d-4552-b93e-581558a2e142",
   "metadata": {},
   "source": [
    "- 三维文字序列\n",
    "\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/06_.png)\n",
    "\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/07_.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b921b7f-c6e4-41c3-8dfa-caf711f1fa01",
   "metadata": {},
   "source": [
    "在这里限于公开课时间，我们就详细展开讲解语音数据和视频数据的状态了。但根据上面所绘制的图像，如果你层曾经学过计算机视觉、且很好地掌握了卷积神经网络，那我想你应该能够自己绘制出音频数据与视频数据的结构。现在我们已经了解了时序数据和文本数据的一般结构，**那请问循环神经网络是用于上述哪种结构的呢**？\n",
    "\n",
    "答案是，都可以！循环神经网络是深度学习中为数不多的、可以在网络架构不变的情况下、同时接受二、三维数据的网络，接下来就让我们看看循环神经网络是如何在序列数据上学习样本与样本之间的顺序的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19512743-72a4-4b76-9b8f-842a159d77f3",
   "metadata": {},
   "source": [
    "## 3 循环神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6507192-1ecf-4e23-ad52-4964343fea0f",
   "metadata": {},
   "source": [
    "循环神经网络（Recurrent Neural Network）是自然语言处理领域的入门级深度学习算法，也是序列数据处理方法的经典代表作，它开创了“记忆”方式、让神经网络可以学习样本之间的关联、它可以处理时间、文字、音频数据，也可以执行NLP领域最为经典的情感分析、机器翻译等工作。在NLP领域，循环神经网络是GRU、LSTM以及许多经典算法的基础、更对我们理解transformer结构有巨大的帮助，因此即便在Transformer和大语言模型统治前沿算法战场的今天，我们依然需要学习RNN算法。RNN就仿佛机器学习中的逻辑回归算法一般，是打开NLP领域大门的钥匙。今天我们就一起来看看RNN的基本逻辑和实现手段。\n",
    "\n",
    "<font color=\"red\">【注意】在学习循环网络之前，请确保你已对基础的深度神经网络（DNN）有深刻的理解，确保你了解所有经典优化层、理解梯度下降过程、理解正向反向传播过程、理解链式法则、神经网络如何训练、深度神经网络如何输入输出等基础知识。<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5e9094-9dbd-4c7e-855c-79842a2f2139",
   "metadata": {},
   "source": [
    "- 循环神经网络的基本架构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb162d3-3fb5-4762-859b-f95f3d72fd5f",
   "metadata": {},
   "source": [
    "如果你去找寻网络上的各种资源，你会惊讶地发现循环神经网络有各种各样复杂的公式表示和图像表示方法。然而，**光从网络架构来说，循环神经网络与深度神经网络是完全一致的**。\n",
    "\n",
    "首先，循环神经网络由输入层、隐藏层和输出层构成，并且这三类层都是线性层。和深度神经网络中的线性层一样，输入层的神经元个数由输入数据的特征数量决定，隐藏层数量和隐藏层上神经元的个数都可自己设置，而输出层的神经元数量则需要根据输出的任务目标进行设置。以下面的数据为例，现在我们将每个单词都编码成了5个特征构成的词向量，因此输入层就会需要5个神经元，我们将该文字数据输入循环神经网络执行三分类的“情感分类”任务（三分类分别是[积极，消极，中性]），那输出层就会需要三个神经元。假设有一个隐藏层，而隐藏层上有2个神经元，一个最为简单的循环网络的网络结构如下：\n",
    "\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/08_.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe0f4f2-a6df-4c7d-adb6-e453e498283c",
   "metadata": {},
   "source": [
    "在这个结构中，激活函数的设置、神经元的连接方式等都与深度神经网络一致，因此循环神经网络在网络构建方面没有太多可以深究的内容，循环网络真正精彩的地方在于其**创造了全新的数据流**，我们来具体看一下——\n",
    "\n",
    "- 循环神经网络的数据流\n",
    "\n",
    "当我们将数据输入深度神经网络时，一个神经元会一次性处理一列数据，5个神经元会涵盖整张表的数据，在一次正向传播中深度神经网络就会接触到完整的一张数据表。这种方式计算效率很高，同时矩阵计算也很简单：输入结构为（9，5），中间层输出为（9，2），最终输出结果为（9，3），整个计算过程完全只考虑每个单词的特征之间的转换（5➡️2➡️3），而完全忽略“单词与单词之间的联系”，毕竟输入9个单词，输出9个单词，并没有对单词之间的关系进行任何学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da603a87-df79-4f23-bf3b-3a1f56f25bca",
   "metadata": {},
   "source": [
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/09.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d06b0ba-68ea-4985-8b1c-f6a2e7d5d147",
   "metadata": {},
   "source": [
    "但是，在循环神经网络当中就不一样了。虽然是一模一样的网络结构，但当我们将数据输入到循环神经网络时，一个神经元一次性只会处理一个单词的一个数据，5个神经元会覆盖当前单词的5个特征，在一次正向传播中，循环神经网络只会接触到一个单词的全部信息，而不会接触到整张表。\n",
    "\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a0e832-bb47-46ef-a012-2498921bfb95",
   "metadata": {},
   "source": [
    "如果这样的话，岂不是要一行一行处理数据了？没错，没错，虽然非常颠覆神经网络当中对效率的根本追求，但循环神经网络是一个单词、一个单词处理文本数据，一个时间点、一个时间点处理时序数据的。具体过程如下：\n",
    "\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/11.png)\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/12.png)\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f032f6-c7cb-4c49-8259-7ee8417519dd",
   "metadata": {},
   "source": [
    "如果一次正向传播只处理一行数据，那对于结构为（vocab_size，input_dimension）的文字数据来说，就需要在同一个网络上进行vocab_size次正向传播。同样的，对于结构为（time_step，input_dimension）的时间序列数据来说，就需要在同一个网络上进行time_step次正向传播。在循环神经网络中，**vocab_size和time_step这个维度可以统称为sequence_length，同时还有一个更常见的名字叫做时间步，对任意数据来说，循环神经网络都需要进行时间步次正向传播，而每个时间步上是一个单词或一个时间点的数据**。\n",
    "\n",
    "基于这样的数据流设置，循环神经网络构建了自己的**灵魂结构：循环数据流**。在多次进行正向传播的过程中，循环神经网络会将每个单词的信息向下传递给下一个单词，从而让网络在处理下一个单词时还能够“记得”上一个单词的信息。\n",
    "\n",
    "如下图所示，在$T_{t-1}$时间步上时，循环网络处理了一个单词，此时隐藏层上输出的中间变量$H_{t-1}$会走向两条数据流，一条数据流是继续向输出层的方向正向传播，另一条则流向了下一个时间步的隐藏层。在$T_{t}$时间步时，隐藏层会结合当前正向传播的输入层传入的$X_t$和上个时间不的隐藏层传来的中间变量$H_{t-1}$共同计算当前隐藏层的输出$H_{t}$。如此，$H_{t}$当中就包含了上一个单词的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf099f3-6fb5-4aeb-b5d7-aab0b26a4f9d",
   "metadata": {},
   "source": [
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6097da0f-a013-42dc-9bba-bef458319bf7",
   "metadata": {},
   "source": [
    "使用数学公式表示如下：\n",
    "\n",
    "$$H_t = f(W_{hh}H_{t-1} + W_{xh}X_t)$$\n",
    "$$ = f(W_{hh}f(W_{hh}H_{t-2} + W_{xh}X_{t-1}) + W_{xh}X_t)$$\n",
    "$$ = f(W_{hh}f(W_{hh}f(W_{hh}H_{t-3} + W_{xh}X_{t-2}) + W_{xh}X_{t-1}) + W_{xh}X_t)$$\n",
    "$$ \\vdots$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242bd3aa-0b4a-41c2-be51-b2b58dc16c63",
   "metadata": {},
   "source": [
    "使用架构图表示，则可表示如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77617633-546e-4c19-9f5d-a27b64740ed3",
   "metadata": {},
   "source": [
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f515f89e-b107-4a80-8da1-859657a7678a",
   "metadata": {},
   "source": [
    "利用这种方式，只要进行vocal_size次向前传播，并且每次都将上一个时间步中隐藏层上诞生的中间变量传递给下一个时间步的隐藏层，整个网络就能在全部的正向传播完成后获得整个句子上的全部信息。在这个过程中，我们在同一个网络上不断运行正向传播，**此过程在神经网络结构上是循环，在数学逻辑上是递归，这也是循环神经网络名称的由来**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40ca99a-d964-4730-8898-c55f7dcee7f7",
   "metadata": {},
   "source": [
    "- 效率问题与RNN的权值共享"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df2bb86-bfa8-44c0-b811-db84e749dfb6",
   "metadata": {},
   "source": [
    "现在你已经知道循环网络的数据流和基本结构了，但我们还面临一个巨大的问题——效率。刚才我们以一张表为例讲解了循环神经网络的迭代过程，但循环网络在实际应用时可能面临batch_size张表单，如果每张表单都需要一行一行进行向前传播的话，那循环神经网络运行一次需要（batch_size * sequence_length）次向前传播，这样整个网络的运行效率必然是非常非常低的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc494f45-2df3-443b-8c91-f48aff251508",
   "metadata": {},
   "source": [
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/06_.png)\n",
    "\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/07_.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1e2691-48a6-41c1-83c2-ed119cb8169e",
   "metadata": {},
   "source": [
    "幸运的是，事实上这个问题并不存在。在现实中使用循环神经网络的时候，我们所使用的输入数据结构往往是三维时间或三维文字数据，也就是说数据中大概率会包括不止一张时序二维表、会包括不止一个句子或一个段落。之前我们提到过，循环神经网络要顺利运行的前提是所有的句子/时间序列被处理成同等的长度，因此实际上每张二维表需要循环的时间步数量是相等的，因此在实际训练的时候循环神经网络是会一次性将所有的batch_size张二维表的第一行数据都放入神经元进行处理，故而RNN并不需要对每张表单一一处理，而是对全部表单的每一行进行一一处理，所以最终循环神经网络只会进行sequence_length次向前传播，所有的batch是共享权重的。\n",
    "\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8494362d-1619-4585-8f2c-d7b06ef24104",
   "metadata": {},
   "source": [
    "如果将三维数据看作是一个立方体，那循环神经网络就是一次性处理位于最上层的一整个平面的数据，因此循环神经网络一次性处理的数据结构与深度神经网络一样都是二维的，只不过这个二维数据不是（vocal_size，input_dimension）结构，而是（batch_size，input_dimension）结构罢了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd89a81-b910-4e0f-b84e-71f942ae74de",
   "metadata": {},
   "source": [
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a0ca39-39e1-4db7-af27-9e502c07ddfb",
   "metadata": {},
   "source": [
    "- 循环网络的输入数据结构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce70febc-62fe-4a1c-b208-3b27bdaddb7c",
   "metadata": {},
   "source": [
    "讲到这里，我相信你已经非常了解循环神经网络的基本结构和巧妙之处了，在开始进行循环神经网络的实现之前，我们还需要解决最后一个问题：明确循环神经网络的输入数据结构。在之前的课程中我们提到过，循环神经网络是为数不多的、能够在不改变网络结构情况下同时处理二维数据和三维数据的网络，**但在PyTorch或tensorflow这样的深度学习框架的要求下，循环神经网络的输入结构一律为三维数据**。通常来说，最常见的结构就是之前我们提到过无数次的（batch_size，vocal_size，input_dimension），且循环是在vocal_size维度进行。不过，如果你曾经自学循环神经网络，或找寻过其他相关的材料，那你可能会发现，在某些材料当中，循环神经网络的输入被描述为（vocal_size，batch_size，input_dimension）结构。事实上，这两种结构是同一种结构，我们来看："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3908f9-70d5-4969-875d-a4c5e66229af",
   "metadata": {},
   "source": [
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b8549d-f410-412b-b0da-87e877d3b77d",
   "metadata": {},
   "source": [
    "普通的结构（batch_size，vocal_size，input_dimension）如左图，此时循环神经网络会在vocal_size这一维度上循环，执行vocal_size个时间步的正向传播、即从上至下不断处理面向上方的二维表单（虚线标注处）。但立方体是可以被旋转的，当我们将立方体旋转一个角度，即需要处理的二维表单由正上方专向正前方时，我们就得到了（vocal_size，batch_size，input_dimension）的数据结构，此时循环神经网络依然是在vocal_size方向进行循环，只不过我们需要处理的表单方向由从上到下变成了从前往后。**因此不难发现，本质上这两种结构是一模一样的，但无论是哪种结构，循环神经网络都必须在时间步的方向，也就是vocal_size的方向进行循环**。在PyTorch中，我们可以通过调节参数「batch_first」帮助RNN认知正确的维度，从而让RNN能够在正确的vocal_size维度上循环。\n",
    "\n",
    "这一点可以由代码佐证："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "888c2ad6-bbf3-412e-9a90-c8cf07a43653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "93c4dc69-0c64-4731-95ca-094e5fc21cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn((3,50,10)) #（batch_size，vocal_size，input_dimension）\n",
    "rnn1 = nn.RNN(input_size=10,hidden_size=20,batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0cf4f337-6cf8-40cc-b2be-2deb6cbaaaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02182292938232422\n"
     ]
    }
   ],
   "source": [
    "#此时vocal_size是500，因此会循环500次\n",
    "start = time.time()\n",
    "outputs1, hn1 = rnn1(inputs)\n",
    "spend = time.time() - start\n",
    "print(spend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eb6f7e6b-cc2e-4c82-9938-027493966e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#此时rnn会认知数据结构为（vocal_size，batch_size，input_dimension）\n",
    "inputs = torch.randn((3,50,10))\n",
    "rnn2 = nn.RNN(input_size=10,hidden_size=20,batch_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "71218ff0-78f7-40de-a18c-01cb022ca019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002652883529663086\n"
     ]
    }
   ],
   "source": [
    "#此时vocal_size是3，因此会循环3次\n",
    "start = time.time()\n",
    "outputs2, hn2 = rnn2(inputs)\n",
    "spend = time.time() - start\n",
    "print(spend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2348759e-90a4-437c-94eb-d5f38d48db5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
